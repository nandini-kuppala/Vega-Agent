{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da32c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded, waiting for job elements...\n",
      "Scrolling iteration 1/3...\n",
      "Scrolling iteration 2/3...\n",
      "Scrolling iteration 3/3...\n",
      "Found 60 job listings\n",
      "Saved 60 jobs to herkey_jobs.json\n",
      "\n",
      "Recommended jobs for Jane Doe:\n",
      "1. Software Engineer at Finastra (Pune | Work From Office)\n",
      "   Skills: C# • Javascript +8\n",
      "\n",
      "2. Software Engineer at Finastra (Pune | Work From Office)\n",
      "   Skills: C# • Javascript +8\n",
      "\n",
      "3. Internship | Work from Home at BTS360 (formerly GetSup... (Bangalore | Work From Home)\n",
      "   Skills: Customer Service • Sales +1\n",
      "\n",
      "4. Work from Home Internship at BTS360 (formerly GetSup... (Bangalore | Work From Home)\n",
      "   Skills: Sales • Client Acquisition\n",
      "\n",
      "5. English Trainer at FRND App (Any | Work From Home)\n",
      "   Skills: Teaching English • Communication Skill +2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "\n",
    "class HerkeyJobScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "    def scrape_jobs(self, url: str = \"https://www.herkey.com/jobs\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape job listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more jobs)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of job dictionaries containing details of each job\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            print(\"Page loaded, waiting for job elements...\")\n",
    "            \n",
    "            # Wait for job elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='job-details']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more jobs\n",
    "            for i in range(max_scroll):\n",
    "                print(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get all job divs\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='job-details']\")\n",
    "            print(f\"Found {len(job_elements)} job listings\")\n",
    "            \n",
    "            jobs = []\n",
    "            for job_element in job_elements:\n",
    "                try:\n",
    "                    # Extract job details using data-test-id attributes\n",
    "                    job_title = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='job-title']\").text\n",
    "                    company_name = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='company-name']\").text\n",
    "                    \n",
    "                    # Get location, work type and experience\n",
    "                    location_info = job_element.find_element(By.CSS_SELECTOR, \"p.capitalize\").text\n",
    "                    \n",
    "                    # Parse location info - typically in format \"City | work type | experience\"\n",
    "                    location_parts = location_info.split('|')\n",
    "                    location = location_parts[0].strip() if len(location_parts) > 0 else \"\"\n",
    "                    work_type = location_parts[1].strip() if len(location_parts) > 1 else \"\"\n",
    "                    experience = location_parts[2].strip() if len(location_parts) > 2 else \"\"\n",
    "                    \n",
    "                    # Try to get skills (not all jobs may have this)\n",
    "                    try:\n",
    "                        skills_element = job_element.find_element(By.CSS_SELECTOR, \"span.capitalize\")\n",
    "                        skills = skills_element.text\n",
    "                    except:\n",
    "                        skills = \"\"\n",
    "                    \n",
    "                    # Check if the job has \"Easy Apply\" option\n",
    "                    try:\n",
    "                        apply_button = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='apply-job']\")\n",
    "                        easy_apply = \"Easy Apply\" in apply_button.text\n",
    "                    except:\n",
    "                        easy_apply = False\n",
    "                    \n",
    "                    # Get company logo URL if available\n",
    "                    try:\n",
    "                        logo_element = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='company-logo'] img\")\n",
    "                        company_logo = logo_element.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        company_logo = \"\"\n",
    "                    \n",
    "                    # Create job dictionary\n",
    "                    job = {\n",
    "                        \"title\": job_title,\n",
    "                        \"company\": company_name,\n",
    "                        \"location\": location,\n",
    "                        \"work_type\": work_type,\n",
    "                        \"experience\": experience,\n",
    "                        \"skills\": skills,\n",
    "                        \"easy_apply\": easy_apply,\n",
    "                        \"company_logo\": company_logo\n",
    "                    }\n",
    "                    \n",
    "                    jobs.append(job)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting job details: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, jobs: List[Dict], filename: str = \"herkey_jobs.json\"):\n",
    "        \"\"\"Save scraped jobs to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(jobs, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(jobs)} jobs to {filename}\")\n",
    "    \n",
    "    def job_recommendation_data(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process job data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes skills, experience and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_jobs = []\n",
    "        \n",
    "        for job in jobs:\n",
    "            processed_job = job.copy()\n",
    "            \n",
    "            # Extract years of experience as a numeric value\n",
    "            exp_text = job.get(\"experience\", \"\")\n",
    "            years_match = re.search(r'(\\d+)-(\\d+)\\s*Yr', exp_text)\n",
    "            if years_match:\n",
    "                min_exp = int(years_match.group(1))\n",
    "                max_exp = int(years_match.group(2))\n",
    "                processed_job[\"min_experience\"] = min_exp\n",
    "                processed_job[\"max_experience\"] = max_exp\n",
    "                processed_job[\"avg_experience\"] = (min_exp + max_exp) / 2\n",
    "            else:\n",
    "                processed_job[\"min_experience\"] = None\n",
    "                processed_job[\"max_experience\"] = None\n",
    "                processed_job[\"avg_experience\"] = None\n",
    "            \n",
    "            # Process skills into a list\n",
    "            skills_text = job.get(\"skills\", \"\")\n",
    "            # Remove the bullet points and '+n' suffix\n",
    "            skills_text = re.sub(r'\\+\\d+', '', skills_text)\n",
    "            skills_list = [skill.strip() for skill in skills_text.split('•') if skill.strip()]\n",
    "            processed_job[\"skills_list\"] = skills_list\n",
    "            \n",
    "            # Standardize work type\n",
    "            work_type = job.get(\"work_type\", \"\").lower()\n",
    "            if \"remote\" in work_type or \"from home\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"remote\"\n",
    "            elif \"office\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"in-office\"\n",
    "            elif \"hybrid\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"hybrid\"\n",
    "            else:\n",
    "                processed_job[\"work_mode\"] = \"unknown\"\n",
    "            \n",
    "            processed_jobs.append(processed_job)\n",
    "        \n",
    "        return processed_jobs\n",
    "\n",
    "\n",
    "def recommend_jobs(candidate_profile: Dict, jobs: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend jobs based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate skills, experience, etc.\n",
    "        jobs: List of job dictionaries\n",
    "        num_recommendations: Number of jobs to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended job dictionaries\n",
    "    \"\"\"\n",
    "    # This is a simple recommendation algorithm as a placeholder\n",
    "    # In a real implementation, you might use more sophisticated matching or ML\n",
    "    \n",
    "    scored_jobs = []\n",
    "    \n",
    "    for job in jobs:\n",
    "        score = 0\n",
    "        \n",
    "        # Match skills (biggest factor)\n",
    "        candidate_skills = [skill.lower() for skill in candidate_profile.get(\"skills\", [])]\n",
    "        job_skills = [skill.lower() for skill in job.get(\"skills_list\", [])]\n",
    "        \n",
    "        for skill in candidate_skills:\n",
    "            if any(skill in job_skill for job_skill in job_skills):\n",
    "                score += 10\n",
    "        \n",
    "        # Match experience\n",
    "        candidate_exp = candidate_profile.get(\"years_of_experience\", 0)\n",
    "        min_exp = job.get(\"min_experience\")\n",
    "        max_exp = job.get(\"max_experience\")\n",
    "        \n",
    "        if min_exp is not None and max_exp is not None:\n",
    "            if min_exp <= candidate_exp <= max_exp:\n",
    "                score += 5\n",
    "            elif candidate_exp < min_exp:\n",
    "                # Candidate has less experience than required\n",
    "                score -= (min_exp - candidate_exp) * 2\n",
    "            else:\n",
    "                # Candidate is overqualified but might still be interested\n",
    "                score -= min(5, (candidate_exp - max_exp))\n",
    "        \n",
    "        # Match work mode preference\n",
    "        if candidate_profile.get(\"preferred_work_mode\") == job.get(\"work_mode\"):\n",
    "            score += 3\n",
    "        \n",
    "        # Match location preference\n",
    "        candidate_locations = candidate_profile.get(\"preferred_locations\", [])\n",
    "        job_location = job.get(\"location\", \"\").lower()\n",
    "        \n",
    "        if not candidate_locations or any(loc.lower() in job_location for loc in candidate_locations):\n",
    "            score += 2\n",
    "        \n",
    "        scored_jobs.append((score, job))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_jobs.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_jobs = [job for score, job in scored_jobs[:num_recommendations]]\n",
    "    \n",
    "    return recommended_jobs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyJobScraper(headless=False)  # Set to True for headless mode\n",
    "    jobs = scraper.scrape_jobs(max_scroll=3)\n",
    "    \n",
    "    # Save raw job data\n",
    "    scraper.save_to_json(jobs)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_jobs = scraper.job_recommendation_data(jobs)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"years_of_experience\": 3,\n",
    "        \"skills\": [\"React\", \"JavaScript\", \"Node.js\", \"HTML\", \"CSS\"],\n",
    "        \"preferred_work_mode\": \"remote\",\n",
    "        \"preferred_locations\": [\"Bangalore\", \"Remote\"]\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_jobs(example_candidate, processed_jobs)\n",
    "    \n",
    "    print(f\"\\nRecommended jobs for {example_candidate['name']}:\")\n",
    "    for i, job in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {job['title']} at {job['company']} ({job['location']} | {job['work_type']})\")\n",
    "        print(f\"   Skills: {job['skills']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e834d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:16:29,537 - __main__ - INFO - Scraping events from https://events.herkey.com/events\n",
      "c:\\Users\\knand\\OneDrive\\Desktop\\AshaAIBot\\asha\\Lib\\site-packages\\soupsieve\\css_parser.py:876: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(  # noqa: B028\n",
      "2025-04-25 08:16:31,204 - __main__ - INFO - Scraped 8 events\n",
      "2025-04-25 08:16:31,206 - __main__ - INFO - Saved 8 events to herkey_events.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended events for Jane Doe:\n",
      "1. Advance Your Career with a Premier Executive MBA from Great Lakes\n",
      "   Date: 16th Apr, 2025 to 30th Apr, 2025 | Time: 10:00am to 6:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development, event\n",
      "\n",
      "2. SkillReBoot program: Restart Your Career Journey\n",
      "   Date: 21st Apr, 2025 to 20th May, 2025 | Time: 6:00pm to 11:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "3. #STEMTheBias Scholarship - Avail Scholarships up to Rs. 80,000\n",
      "   Date: 3rd Mar, 2025 to 25th Mar, 2025 | Time: 10:00am to 11:59pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "4. LeadHERs in Tech: Tech Meets Talent , Innovation Meets Inspiration\n",
      "   Date: 21st Mar, 2025 | Time: 8:30am to 4:00pm\n",
      "   Type: Offline | Price: Free\n",
      "   Categories: Career Development, Women In Tech, networking\n",
      "\n",
      "5. herShakti\n",
      "   Date: 8th Mar, 2025 to 10th Apr, 2025 | Time: 12:00am to 11:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: JobsForHer foundation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyEventScraper:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the event scraper\"\"\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def _get_page_content(self, url: str) -> str:\n",
    "        \"\"\"Get HTML content from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def scrape_events(self, url: str = \"https://events.herkey.com/events\") -> List[Dict]:\n",
    "        \"\"\"Scrape events from HerKey events page.\"\"\"\n",
    "        logger.info(f\"Scraping events from {url}\")\n",
    "        html_content = self._get_page_content(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        events = []\n",
    "        \n",
    "        # Extract event listings based on the provided HTML structure\n",
    "        event_cards = soup.select(\".event-details-card\")\n",
    "        \n",
    "        for card in event_cards:\n",
    "            try:\n",
    "                event = {}\n",
    "                \n",
    "                # Extract event details based on the actual HTML structure\n",
    "                title_elem = card.select_one(\".card-heading\")\n",
    "                date_elem = card.select_one(\".card-body-data:has(img[src*='calendar'])\")\n",
    "                time_elem = card.select_one(\".card-body-data:has(img[src*='clock'])\")\n",
    "                location_elem = card.select_one(\".card-body-data:has(img[src*='placeholder'])\")\n",
    "                event_type_elem = card.select_one(\"span.mr-1, .card-body-data:has(i.fa-bullseye)\")\n",
    "                category_elem = card.select_one(\".card-body-data:has(img[src*='tag'])\")\n",
    "                \n",
    "                if title_elem:\n",
    "                    title_text = title_elem.get_text(strip=True)\n",
    "                    # Remove any icon text from the title\n",
    "                    if title_text:\n",
    "                        event[\"title\"] = title_text.split(\"Featured\")[0].strip()\n",
    "                \n",
    "                if date_elem:\n",
    "                    date_text = date_elem.get_text(strip=True)\n",
    "                    if date_text:\n",
    "                        event[\"date\"] = date_text\n",
    "                \n",
    "                if time_elem:\n",
    "                    time_text = time_elem.get_text(strip=True)\n",
    "                    if time_text:\n",
    "                        event[\"time\"] = time_text\n",
    "                \n",
    "                if location_elem:\n",
    "                    location_text = location_elem.get_text(strip=True)\n",
    "                    if location_text:\n",
    "                        event[\"location\"] = location_text\n",
    "                \n",
    "                if event_type_elem:\n",
    "                    event_type = event_type_elem.get_text(strip=True)\n",
    "                    if event_type and \"Online\" in event_type or \"Offline\" in event_type:\n",
    "                        event[\"event_type\"] = event_type\n",
    "                \n",
    "                if category_elem:\n",
    "                    # Parse categories (they're in a special format)\n",
    "                    category_links = category_elem.select(\"a\")\n",
    "                    if category_links:\n",
    "                        event[\"categories\"] = [link.text.strip() for link in category_links]\n",
    "                \n",
    "                # Get the registration button type\n",
    "                register_btn = card.select_one(\".register\")\n",
    "                if register_btn:\n",
    "                    event[\"registration_status\"] = \"Open\"\n",
    "                    event[\"registration_text\"] = register_btn.text.strip()\n",
    "                \n",
    "                # Check if it's a paid event\n",
    "                price_elem = card.select_one(\".card-body-data:contains('₹')\")\n",
    "                if price_elem:\n",
    "                    event[\"price\"] = price_elem.text.strip()\n",
    "                else:\n",
    "                    event[\"price\"] = \"Free\"\n",
    "                \n",
    "                # Get event ID and URL\n",
    "                event_id_span = card.select_one(\"span[id]\")\n",
    "                if event_id_span and event_id_span.get('id'):\n",
    "                    event_id = event_id_span.get('id')\n",
    "                    if \"title\" in event:\n",
    "                        event[\"url\"] = f\"{url}/{event['title'].lower().replace(' ', '-')}/{event_id}\"\n",
    "                \n",
    "                # Check if it's a featured event\n",
    "                featured_elem = card.select_one(\"img[src*='Featured'], img[width='22'][height='22']\")\n",
    "                if featured_elem:\n",
    "                    event[\"featured\"] = True\n",
    "                \n",
    "                if event:  # Only add if we found some data\n",
    "                    events.append(event)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting event data: {e}\")\n",
    "                \n",
    "        logger.info(f\"Scraped {len(events)} events\")\n",
    "        return events\n",
    "    \n",
    "    def save_to_json(self, events: List[Dict], filename: str = \"herkey_events.json\"):\n",
    "        \"\"\"Save scraped events to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(events, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(events)} events to {filename}\")\n",
    "    \n",
    "    def process_events_for_recommendation(self, events: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process event data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes categories, dates and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_events = []\n",
    "        \n",
    "        for event in events:\n",
    "            processed_event = event.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = event.get(\"date\", \"\")\n",
    "            try:\n",
    "                # Example format: \"25 Apr 2023\" - adjust pattern as needed\n",
    "                date_obj = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "                processed_event[\"datetime_obj\"] = date_obj\n",
    "                processed_event[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, keep it as is\n",
    "                processed_event[\"is_upcoming\"] = True  # Assume upcoming by default\n",
    "            \n",
    "            # Standardize event type\n",
    "            event_type = event.get(\"event_type\", \"\").lower()\n",
    "            if \"online\" in event_type:\n",
    "                processed_event[\"mode\"] = \"online\"\n",
    "            elif \"offline\" in event_type or \"in-person\" in event_type:\n",
    "                processed_event[\"mode\"] = \"offline\"\n",
    "            else:\n",
    "                processed_event[\"mode\"] = \"unknown\"\n",
    "            \n",
    "            # Standardize categories\n",
    "            categories = event.get(\"categories\", [])\n",
    "            processed_event[\"categories_lower\"] = [cat.lower() for cat in categories]\n",
    "            \n",
    "            # Determine if event is free\n",
    "            price = event.get(\"price\", \"\")\n",
    "            processed_event[\"is_free\"] = price == \"Free\" or \"free\" in price.lower()\n",
    "            \n",
    "            processed_events.append(processed_event)\n",
    "        \n",
    "        return processed_events\n",
    "\n",
    "\n",
    "def recommend_events(candidate_profile: Dict, events: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend events based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate interests, career stage, etc.\n",
    "        events: List of event dictionaries\n",
    "        num_recommendations: Number of events to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended event dictionaries\n",
    "    \"\"\"\n",
    "    # Process events for recommendation if they haven't been processed\n",
    "    if events and \"categories_lower\" not in events[0]:\n",
    "        scraper = HerkeyEventScraper()\n",
    "        events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    scored_events = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "    preferred_mode = candidate_profile.get(\"preferred_event_mode\", \"\").lower()\n",
    "    preferred_locations = [loc.lower() for loc in candidate_profile.get(\"preferred_locations\", [])]\n",
    "    career_stage = candidate_profile.get(\"career_stage\", \"\").lower()\n",
    "    \n",
    "    # Only consider upcoming events\n",
    "    upcoming_events = [event for event in events if event.get(\"is_upcoming\", True)]\n",
    "    \n",
    "    for event in upcoming_events:\n",
    "        score = 0\n",
    "        \n",
    "        # Match interests with event categories\n",
    "        event_categories = event.get(\"categories_lower\", [])\n",
    "        for interest in interests:\n",
    "            if any(interest in category for category in event_categories):\n",
    "                score += 10\n",
    "        \n",
    "        # Match event mode preference (online/offline)\n",
    "        event_mode = event.get(\"mode\", \"unknown\")\n",
    "        if preferred_mode and event_mode == preferred_mode:\n",
    "            score += 5\n",
    "        \n",
    "        # Match location preference if it's an offline event\n",
    "        if event_mode == \"offline\":\n",
    "            location = event.get(\"location\", \"\").lower()\n",
    "            if any(loc in location for loc in preferred_locations):\n",
    "                score += 5\n",
    "        \n",
    "        # Free events might be more appealing\n",
    "        if event.get(\"is_free\", False):\n",
    "            score += 2\n",
    "        \n",
    "        # Featured events might be more relevant/important\n",
    "        if event.get(\"featured\", False):\n",
    "            score += 3\n",
    "        \n",
    "        # Career stage specific events\n",
    "        event_title = event.get(\"title\", \"\").lower()\n",
    "        if career_stage in event_title or any(career_stage in cat for cat in event_categories):\n",
    "            score += 4\n",
    "        \n",
    "        scored_events.append((score, event))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_events.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_events = [event for score, event in scored_events[:num_recommendations]]\n",
    "    \n",
    "    return recommended_events\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyEventScraper()\n",
    "    events = scraper.scrape_events()\n",
    "    \n",
    "    # Save raw event data\n",
    "    scraper.save_to_json(events)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"interests\": [\"Technology\", \"Career Development\", \"Leadership\", \"AI\"],\n",
    "        \"preferred_event_mode\": \"online\",\n",
    "        \"preferred_locations\": [\"Bangalore\", \"Mumbai\", \"Delhi\"],\n",
    "        \"career_stage\": \"mid-level\"\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_events(example_candidate, processed_events)\n",
    "    \n",
    "    print(f\"\\nRecommended events for {example_candidate['name']}:\")\n",
    "    for i, event in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {event['title']}\")\n",
    "        print(f\"   Date: {event['date']} | Time: {event.get('time', 'N/A')}\")\n",
    "        print(f\"   Type: {event.get('event_type', 'N/A')} | Price: {event.get('price', 'N/A')}\")\n",
    "        print(f\"   Categories: {', '.join(event.get('categories', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2111674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:20:11,457 - __main__ - INFO - Page loaded, waiting for group elements...\n",
      "2025-04-25 08:20:12,870 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-04-25 08:20:15,220 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-04-25 08:20:17,434 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-04-25 08:20:19,658 - __main__ - INFO - Found 104 group elements\n",
      "2025-04-25 08:20:22,126 - __main__ - INFO - Saved 103 groups to herkey_groups.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended groups for Jane Doe:\n",
      "1. Women Engineers\n",
      "   Type: public | Members: 485\n",
      "   Category: development | Level: all\n",
      "   Featured: No\n",
      "\n",
      "2. Full Stack (MERN) Developer Program\n",
      "   Type: private | Members: N/A\n",
      "   Category: development | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "3. Ambassadors Club\n",
      "   Type: private | Members: 4129\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "4. Women on a Career Break\n",
      "   Type: public | Members: 36445\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "5. Women's March\n",
      "   Type: public | Members: 6066\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyGroupScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the group scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_groups(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape group listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more groups)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of group dictionaries containing details of each group\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            # Wait for group elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='featured-group']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get page source after all content is loaded\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            \n",
    "            # Find all group container elements\n",
    "            group_elements = soup.select(\"div.MuiGrid-container.css-1d3bbye\")\n",
    "            logger.info(f\"Found {len(group_elements)} group elements\")\n",
    "            \n",
    "            groups = []\n",
    "            for group_element in group_elements:\n",
    "                try:\n",
    "                    group = {}\n",
    "                    \n",
    "                    # Extract group name\n",
    "                    name_elem = group_element.select_one(\"[data-test-id='group-name']\")\n",
    "                    if name_elem:\n",
    "                        group[\"name\"] = name_elem.text.strip()\n",
    "                    \n",
    "                    # Extract group type (private/public)\n",
    "                    type_elem = group_element.select_one(\"[data-test-id='group-type']\")\n",
    "                    if type_elem:\n",
    "                        group[\"type\"] = type_elem.text.strip().lower()\n",
    "                    \n",
    "                    # Extract member count\n",
    "                    members_elem = group_element.select_one(\"[data-test-id='group-members-count']\")\n",
    "                    if members_elem:\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        # Extract number from text like \"43 Members\"\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            group[\"member_count\"] = int(members_match.group(1))\n",
    "                    \n",
    "                    # Check if it's a featured group\n",
    "                    featured_elem = group_element.select_one(\"[data-test-id='featured-icon']\")\n",
    "                    group[\"featured\"] = featured_elem is not None\n",
    "                    \n",
    "                    # Extract group icon URL\n",
    "                    icon_elem = group_element.select_one(\"[data-test-id='group-icon']\")\n",
    "                    if icon_elem and icon_elem.has_attr('src'):\n",
    "                        group[\"icon_url\"] = icon_elem['src']\n",
    "                    \n",
    "                    # Extract banner image if available\n",
    "                    banner_elem = group_element.select_one(\".css-12c20jy img\")\n",
    "                    if banner_elem and banner_elem.has_attr('src'):\n",
    "                        group[\"banner_url\"] = banner_elem['src']\n",
    "                    \n",
    "                    # Extract group category/topic if available\n",
    "                    # Note: This might not be directly visible in the HTML snippet provided\n",
    "                    # We'll use a more generic approach to try and find it\n",
    "                    category_elem = group_element.select_one(\".MuiTypography-root.capitalize\")\n",
    "                    if category_elem:\n",
    "                        group[\"category\"] = category_elem.text.strip()\n",
    "                    \n",
    "                    # Extract join button status\n",
    "                    join_btn = group_element.select_one(\"[data-test-id='join-btn'] button\")\n",
    "                    if join_btn:\n",
    "                        group[\"joinable\"] = True\n",
    "                        group[\"join_button_text\"] = join_btn.text.strip()\n",
    "                    else:\n",
    "                        group[\"joinable\"] = False\n",
    "                    \n",
    "                    if \"name\" in group:  # Only add if we have at least a name\n",
    "                        groups.append(group)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group data: {e}\")\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, groups: List[Dict], filename: str = \"herkey_groups.json\"):\n",
    "        \"\"\"Save scraped groups to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(groups, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(groups)} groups to {filename}\")\n",
    "    \n",
    "    def process_groups_for_recommendation(self, groups: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process group data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes relevant fields\n",
    "        \"\"\"\n",
    "        processed_groups = []\n",
    "        \n",
    "        for group in groups:\n",
    "            processed_group = group.copy()\n",
    "            \n",
    "            # Extract topics/skills from group name where possible\n",
    "            name = group.get(\"name\", \"\").lower()\n",
    "            \n",
    "            # Extract potential skills/technologies\n",
    "            tech_keywords = [\n",
    "                \"javascript\", \"python\", \"java\", \"c++\", \"ruby\", \"php\", \"golang\", \"react\", \n",
    "                \"angular\", \"vue\", \"node\", \"express\", \"django\", \"flask\", \"spring\", \"bootstrap\",\n",
    "                \"html\", \"css\", \"sql\", \"nosql\", \"mongodb\", \"mysql\", \"postgresql\", \"oracle\",\n",
    "                \"aws\", \"azure\", \"gcp\", \"cloud\", \"devops\", \"data science\", \"machine learning\",\n",
    "                \"ai\", \"artificial intelligence\", \"blockchain\", \"iot\", \"mobile\", \"android\",\n",
    "                \"ios\", \"swift\", \"kotlin\", \"flutter\", \"react native\", \"full stack\", \"frontend\",\n",
    "                \"backend\", \"ui\", \"ux\", \"design\", \"product\", \"agile\", \"scrum\", \"kanban\",\n",
    "                \"mern\", \"mean\", \"lamp\", \"microservices\", \"docker\", \"kubernetes\", \"jenkins\",\n",
    "                \"ci/cd\", \"testing\", \"qa\", \"security\", \"cyber security\", \"data engineering\",\n",
    "                \"big data\", \"hadoop\", \"spark\", \"tableau\", \"power bi\", \"data visualization\"\n",
    "            ]\n",
    "            \n",
    "            # Look for tech keywords in the group name\n",
    "            found_keywords = [keyword for keyword in tech_keywords if keyword in name]\n",
    "            processed_group[\"tech_keywords\"] = found_keywords\n",
    "            \n",
    "            # Determine if group is for beginners, intermediate, or advanced\n",
    "            level_indicators = {\n",
    "                \"beginner\": [\"beginner\", \"basic\", \"fundamental\", \"101\", \"intro\", \"start\"],\n",
    "                \"intermediate\": [\"intermediate\", \"mid-level\"],\n",
    "                \"advanced\": [\"advanced\", \"expert\", \"professional\", \"master\", \"senior\"]\n",
    "            }\n",
    "            \n",
    "            for level, indicators in level_indicators.items():\n",
    "                if any(indicator in name for indicator in indicators):\n",
    "                    processed_group[\"level\"] = level\n",
    "                    break\n",
    "            else:\n",
    "                processed_group[\"level\"] = \"all\"  # Default if no level is detected\n",
    "            \n",
    "            # Add group category based on name (simplified)\n",
    "            if any(term in name for term in [\"developer\", \"coding\", \"programming\", \"engineer\", \"mern\", \"stack\"]):\n",
    "                processed_group[\"category\"] = \"development\"\n",
    "            elif any(term in name for term in [\"design\", \"ui\", \"ux\", \"user experience\"]):\n",
    "                processed_group[\"category\"] = \"design\"\n",
    "            elif any(term in name for term in [\"data\", \"analytics\", \"science\", \"machine learning\", \"ai\"]):\n",
    "                processed_group[\"category\"] = \"data_science\"\n",
    "            elif any(term in name for term in [\"management\", \"leader\", \"agile\", \"scrum\", \"product\"]):\n",
    "                processed_group[\"category\"] = \"management\"\n",
    "            elif any(term in name for term in [\"testing\", \"qa\", \"quality\"]):\n",
    "                processed_group[\"category\"] = \"testing\"\n",
    "            elif any(term in name for term in [\"devops\", \"cloud\", \"aws\", \"azure\", \"gcp\"]):\n",
    "                processed_group[\"category\"] = \"devops\"\n",
    "            else:\n",
    "                processed_group[\"category\"] = \"other\"\n",
    "            \n",
    "            processed_groups.append(processed_group)\n",
    "        \n",
    "        return processed_groups\n",
    "\n",
    "\n",
    "def recommend_groups(candidate_profile: Dict, groups: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend groups based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate skills, experience level, etc.\n",
    "        groups: List of group dictionaries\n",
    "        num_recommendations: Number of groups to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended group dictionaries\n",
    "    \"\"\"\n",
    "    # Process groups for recommendation if they haven't been processed\n",
    "    if groups and \"tech_keywords\" not in groups[0]:\n",
    "        scraper = HerkeyGroupScraper()\n",
    "        groups = scraper.process_groups_for_recommendation(groups)\n",
    "    \n",
    "    scored_groups = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    skills = [skill.lower() for skill in candidate_profile.get(\"skills\", [])]\n",
    "    experience_level = candidate_profile.get(\"experience_level\", \"\").lower()\n",
    "    \n",
    "    # Map experience level to group levels\n",
    "    level_mapping = {\n",
    "        \"entry\": [\"beginner\", \"all\"],\n",
    "        \"junior\": [\"beginner\", \"intermediate\", \"all\"],\n",
    "        \"mid-level\": [\"intermediate\", \"all\"],\n",
    "        \"senior\": [\"intermediate\", \"advanced\", \"all\"],\n",
    "        \"lead\": [\"advanced\", \"all\"]\n",
    "    }\n",
    "    \n",
    "    suitable_levels = level_mapping.get(experience_level, [\"all\"])\n",
    "    \n",
    "    for group in groups:\n",
    "        score = 0\n",
    "        \n",
    "        # Match skills with group tech keywords\n",
    "        group_keywords = group.get(\"tech_keywords\", [])\n",
    "        for skill in skills:\n",
    "            if any(skill in keyword for keyword in group_keywords):\n",
    "                score += 10\n",
    "            # Also check if skill appears in the group name\n",
    "            if skill in group.get(\"name\", \"\").lower():\n",
    "                score += 5\n",
    "        \n",
    "        # Match experience level with group level\n",
    "        group_level = group.get(\"level\", \"all\")\n",
    "        if group_level in suitable_levels:\n",
    "            score += 5\n",
    "        \n",
    "        # Featured groups might be more relevant/important\n",
    "        if group.get(\"featured\", False):\n",
    "            score += 3\n",
    "        \n",
    "        # Groups with more members might be more valuable\n",
    "        member_count = group.get(\"member_count\", 0)\n",
    "        if member_count > 100:\n",
    "            score += 3\n",
    "        elif member_count > 50:\n",
    "            score += 2\n",
    "        elif member_count > 10:\n",
    "            score += 1\n",
    "        \n",
    "        # Category matching\n",
    "        candidate_interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "        group_category = group.get(\"category\", \"\").lower()\n",
    "        \n",
    "        if any(interest in group_category for interest in candidate_interests):\n",
    "            score += 5\n",
    "        \n",
    "        scored_groups.append((score, group))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_groups.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_groups = [group for score, group in scored_groups[:num_recommendations]]\n",
    "    \n",
    "    return recommended_groups\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyGroupScraper(headless=False)  # Set to True for headless mode\n",
    "    groups = scraper.scrape_groups(max_scroll=3)\n",
    "    \n",
    "    # Save raw group data\n",
    "    scraper.save_to_json(groups)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_groups = scraper.process_groups_for_recommendation(groups)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"skills\": [\"React\", \"JavaScript\", \"Node.js\", \"MongoDB\", \"Express\"],\n",
    "        \"experience_level\": \"mid-level\",\n",
    "        \"interests\": [\"development\", \"web development\", \"frontend\", \"backend\"]\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_groups(example_candidate, processed_groups)\n",
    "    \n",
    "    print(f\"\\nRecommended groups for {example_candidate['name']}:\")\n",
    "    for i, group in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {group['name']}\")\n",
    "        print(f\"   Type: {group.get('type', 'N/A')} | Members: {group.get('member_count', 'N/A')}\")\n",
    "        print(f\"   Category: {group.get('category', 'N/A')} | Level: {group.get('level', 'N/A')}\")\n",
    "        print(f\"   Featured: {'Yes' if group.get('featured', False) else 'No'}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:36:58,210 - __main__ - INFO - Page loaded, waiting for session elements...\n",
      "2025-04-25 08:36:58,229 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-04-25 08:37:00,240 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-04-25 08:37:02,252 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-04-25 08:37:04,284 - __main__ - INFO - Found 29 session listings\n",
      "2025-04-25 08:37:12,997 - __main__ - INFO - Saved 29 sessions to herkey_sessions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended sessions for John Doe:\n",
      "1. Power BI Masterclass : Calculating Measures with DAX (Part 1)\n",
      "   Date: Today | Time: 10:30 AM\n",
      "   Host: Swati Agarwal\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: power bi, dax\n",
      "\n",
      "2. AccelHERate Delhi 2025 | The Future of Hiring: Merging Tech & Human Touch to Unlock Women Talent\n",
      "   Date: Today | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "3. AccelHERate Pune 2025 | Hiring Reimagined: Balancing Tech & Human Connection to Unlock Women Talent\n",
      "   Date: 16 May 25 | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "4. Paroma Chatterjee , CEO, Revolut India on transforming the Indian Fintech space.\n",
      "   Date: Today | Time: 3:00 PM\n",
      "   Host: Khushboo Ramnane\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "5. Opportunity to Network With Employers And Industry Leaders | HerRising 2025 | Bangalore\n",
      "   Date: 19 Jun 25 | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeySessionScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the session scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_sessions(self, url: str = \"https://www.herkey.com/sessions\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape session listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more sessions)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of session dictionaries containing details of each session\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for session elements...\")\n",
    "            \n",
    "            # Wait for session elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[datatestid='session-card']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more sessions\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get all session cards\n",
    "            session_elements = driver.find_elements(By.CSS_SELECTOR, \"[datatestid='session-card'], [data-test-id='session-card']\")\n",
    "            logger.info(f\"Found {len(session_elements)} session listings\")\n",
    "            \n",
    "            sessions = []\n",
    "            for session_element in session_elements:\n",
    "                try:\n",
    "                    session = {}\n",
    "                    \n",
    "                    # Extract session time, date\n",
    "                    try:\n",
    "                        time_data = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='time-data']\").text\n",
    "                        if time_data:\n",
    "                            # Parse date and time (format: \"24 Apr 25 | 10:30 AM\")\n",
    "                            date_time_parts = time_data.split('|')\n",
    "                            if len(date_time_parts) > 0:\n",
    "                                session[\"date\"] = date_time_parts[0].strip()\n",
    "                            if len(date_time_parts) > 1:\n",
    "                                session[\"time\"] = date_time_parts[1].strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract time data\")\n",
    "                    \n",
    "                    # Extract session title\n",
    "                    try:\n",
    "                        discussion_subject = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-subject'] p\").text\n",
    "                        if discussion_subject:\n",
    "                            session[\"title\"] = discussion_subject\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session title\")\n",
    "                    \n",
    "                    # Extract host info\n",
    "                    try:\n",
    "                        host_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='nav-to-user-profile'] h6\")\n",
    "                        if host_element:\n",
    "                            session[\"host\"] = host_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host name\")\n",
    "                    \n",
    "                    # Extract host headline/role if available\n",
    "                    try:\n",
    "                        headline_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='headline']\")\n",
    "                        if headline_element and headline_element.text.strip():\n",
    "                            session[\"host_headline\"] = headline_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host headline\")\n",
    "                    \n",
    "                    # Get host stage/level\n",
    "                    try:\n",
    "                        stage_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-stage'] span\")\n",
    "                        if stage_element:\n",
    "                            session[\"host_stage\"] = stage_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host stage\")\n",
    "                    \n",
    "                    # Get participant count\n",
    "                    try:\n",
    "                        participant_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-pic']\")\n",
    "                        participant_text = participant_element.text\n",
    "                        if participant_text and participant_text.startswith(\"+\"):\n",
    "                            session[\"participant_count\"] = int(participant_text.replace(\"+\", \"\").strip())\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract participant count\")\n",
    "                    \n",
    "                    # Get session type (past/upcoming)\n",
    "                    try:\n",
    "                        session_type = session_element.get_attribute(\"data-sessiontype\")\n",
    "                        if session_type:\n",
    "                            session[\"session_type\"] = session_type\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session type\")\n",
    "                    \n",
    "                    # Get session ID\n",
    "                    try:\n",
    "                        session_id = session_element.get_attribute(\"data-id\")\n",
    "                        if session_id:\n",
    "                            session[\"id\"] = session_id\n",
    "                            # Construct URL\n",
    "                            if \"title\" in session:\n",
    "                                slug = session[\"title\"].lower().replace(\" \", \"-\")\n",
    "                                session[\"url\"] = f\"{url}/{slug}/{session_id}\"\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session ID\")\n",
    "                    \n",
    "                    # Check if it's a video session\n",
    "                    try:\n",
    "                        video_element = session_element.find_element(By.CSS_SELECTOR, \"div[style*='youtube.com']\")\n",
    "                        if video_element:\n",
    "                            session[\"is_video\"] = True\n",
    "                            video_url = video_element.get_attribute(\"style\")\n",
    "                            # Extract YouTube video ID\n",
    "                            youtube_match = re.search(r'youtube.com/vi/([^/]+)/', video_url)\n",
    "                            if youtube_match:\n",
    "                                session[\"youtube_id\"] = youtube_match.group(1)\n",
    "                    except:\n",
    "                        session[\"is_video\"] = False\n",
    "                    \n",
    "                    # Get session status/action button text\n",
    "                    try:\n",
    "                        status_button = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-status-button'] button p\")\n",
    "                        if status_button:\n",
    "                            session[\"action\"] = status_button.text.strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract action button text\")\n",
    "                    \n",
    "                    if session:  # Only add if we found some data\n",
    "                        sessions.append(session)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting session data: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return sessions\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, sessions: List[Dict], filename: str = \"herkey_sessions.json\"):\n",
    "        \"\"\"Save scraped sessions to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sessions, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(sessions)} sessions to {filename}\")\n",
    "    \n",
    "    def process_sessions_for_recommendation(self, sessions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process session data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes dates, topics and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_sessions = []\n",
    "        \n",
    "        for session in sessions:\n",
    "            processed_session = session.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = session.get(\"date\", \"\")\n",
    "            time_str = session.get(\"time\", \"\")\n",
    "            datetime_str = f\"{date_str} {time_str}\".strip()\n",
    "\n",
    "            # In the process_sessions_for_recommendation method\n",
    "            try:\n",
    "                # Try various date formats\n",
    "                date_obj = None  # Initialize date_obj to avoid the UnboundLocalError\n",
    "                \n",
    "                if len(date_str.split()) == 3:  # Format: \"24 Apr 25\"\n",
    "                    date_obj = datetime.strptime(datetime_str, \"%d %b %y %I:%M %p\")\n",
    "                else:\n",
    "                    # Try other formats if the first one fails\n",
    "                    date_formats = [\n",
    "                        \"%d %b %Y %I:%M %p\",  # 24 Apr 2025 10:30 AM\n",
    "                        \"%d %B %Y %I:%M %p\",  # 24 April 2025 10:30 AM\n",
    "                        \"%b %d, %Y %I:%M %p\"  # Apr 24, 2025 10:30 AM\n",
    "                    ]\n",
    "                    \n",
    "                    for date_format in date_formats:\n",
    "                        try:\n",
    "                            date_obj = datetime.strptime(datetime_str, date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                # Check if date_obj was successfully set\n",
    "                if date_obj:\n",
    "                    processed_session[\"datetime_obj\"] = date_obj\n",
    "                    processed_session[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "                else:\n",
    "                    # Handle case where no format worked\n",
    "                    raise ValueError(\"Could not parse date string\")\n",
    "                    \n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, determine upcoming from session_type\n",
    "                if session.get(\"session_type\") == \"upcoming\":\n",
    "                    processed_session[\"is_upcoming\"] = True\n",
    "                else:\n",
    "                    processed_session[\"is_upcoming\"] = False\n",
    "            \n",
    "            # Extract topics from title\n",
    "            title = session.get(\"title\", \"\").lower()\n",
    "            topics = []\n",
    "            \n",
    "            # Common tech topics to check in the title\n",
    "            tech_topics = [\n",
    "                \"python\", \"javascript\", \"react\", \"angular\", \"vue\", \"node\", \"java\", \"c++\", \"c#\", \n",
    "                \"php\", \"ruby\", \"golang\", \"rust\", \"swift\", \"kotlin\", \"sql\", \"nosql\", \"mongodb\",\n",
    "                \"database\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"devops\",\n",
    "                \"ai\", \"machine learning\", \"data science\", \"deep learning\", \"nlp\", \"computer vision\",\n",
    "                \"blockchain\", \"iot\", \"mobile\", \"web\", \"frontend\", \"backend\", \"fullstack\", \"ui\", \"ux\",\n",
    "                \"testing\", \"qa\", \"security\", \"agile\", \"scrum\", \"kanban\", \"product management\",\n",
    "                \"power bi\", \"tableau\", \"excel\", \"analytics\", \"big data\", \"hadoop\", \"spark\", \n",
    "                \"cybersecurity\", \"networking\", \"seo\", \"digital marketing\", \"dax\"\n",
    "            ]\n",
    "            \n",
    "            for topic in tech_topics:\n",
    "                if topic in title:\n",
    "                    topics.append(topic)\n",
    "            \n",
    "            processed_session[\"extracted_topics\"] = topics\n",
    "            \n",
    "            # Determine if session is technical or soft-skills\n",
    "            tech_indicators = [\"programming\", \"code\", \"developer\", \"software\", \"tech\", \"data\", \n",
    "                               \"engineering\", \"algorithm\", \"system\", \"database\", \"cloud\", \"devops\"]\n",
    "            \n",
    "            softskill_indicators = [\"career\", \"leadership\", \"management\", \"communication\",\n",
    "                                   \"soft skill\", \"interview\", \"resume\", \"cv\", \"personal\",\n",
    "                                   \"growth\", \"mindset\", \"wellbeing\", \"mental health\"]\n",
    "            \n",
    "            tech_score = sum(1 for indicator in tech_indicators if indicator in title)\n",
    "            softskill_score = sum(1 for indicator in softskill_indicators if indicator in title)\n",
    "            \n",
    "            if tech_score > softskill_score:\n",
    "                processed_session[\"session_category\"] = \"technical\"\n",
    "            elif softskill_score > tech_score:\n",
    "                processed_session[\"session_category\"] = \"soft skills\"\n",
    "            else:\n",
    "                processed_session[\"session_category\"] = \"general\"\n",
    "            \n",
    "            # Determine experience level based on title\n",
    "            if any(x in title for x in [\"beginner\", \"basic\", \"introduction\", \"101\", \"fundamentals\"]):\n",
    "                processed_session[\"experience_level\"] = \"beginner\"\n",
    "            elif any(x in title for x in [\"advanced\", \"expert\", \"mastery\", \"professional\"]):\n",
    "                processed_session[\"experience_level\"] = \"advanced\"\n",
    "            elif any(x in title for x in [\"intermediate\", \"part 2\", \"level 2\"]):\n",
    "                processed_session[\"experience_level\"] = \"intermediate\"\n",
    "            else:\n",
    "                processed_session[\"experience_level\"] = \"all levels\"\n",
    "            \n",
    "            processed_sessions.append(processed_session)\n",
    "        \n",
    "        return processed_sessions\n",
    "\n",
    "\n",
    "def recommend_sessions(candidate_profile: Dict, sessions: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend sessions based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate interests, experience level, etc.\n",
    "        sessions: List of session dictionaries\n",
    "        num_recommendations: Number of sessions to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended session dictionaries\n",
    "    \"\"\"\n",
    "    # Process sessions for recommendation if they haven't been processed\n",
    "    if sessions and \"extracted_topics\" not in sessions[0]:\n",
    "        scraper = HerkeySessionScraper()\n",
    "        sessions = scraper.process_sessions_for_recommendation(sessions)\n",
    "    \n",
    "    scored_sessions = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "    skills = [skill.lower() for skill in candidate_profile.get(\"skills\", [])]\n",
    "    career_stage = candidate_profile.get(\"career_stage\", \"\").lower()\n",
    "    experience_level = candidate_profile.get(\"experience_level\", \"intermediate\").lower()\n",
    "    career_goals = [goal.lower() for goal in candidate_profile.get(\"career_goals\", [])]\n",
    "    \n",
    "    # Combine interests and skills for broader matching\n",
    "    all_interests = interests + skills\n",
    "    \n",
    "    # Only consider upcoming sessions by default unless specified otherwise\n",
    "    if candidate_profile.get(\"include_past_sessions\", False):\n",
    "        filtered_sessions = sessions\n",
    "    else:\n",
    "        filtered_sessions = [session for session in sessions if session.get(\"is_upcoming\", True)]\n",
    "    \n",
    "    for session in filtered_sessions:\n",
    "        score = 0\n",
    "        \n",
    "        # Match extracted topics with interests and skills\n",
    "        for topic in session.get(\"extracted_topics\", []):\n",
    "            if any(interest in topic or topic in interest for interest in all_interests):\n",
    "                score += 10\n",
    "        \n",
    "        # Match session title keywords with interests and skills\n",
    "        title = session.get(\"title\", \"\").lower()\n",
    "        for interest in all_interests:\n",
    "            if interest in title:\n",
    "                score += 5\n",
    "        \n",
    "        # Match experience level\n",
    "        session_level = session.get(\"experience_level\", \"all levels\")\n",
    "        if session_level == \"all levels\" or session_level == experience_level:\n",
    "            score += 3\n",
    "        elif (session_level == \"beginner\" and experience_level in [\"intermediate\", \"advanced\"]) or \\\n",
    "             (session_level == \"intermediate\" and experience_level == \"advanced\"):\n",
    "            # Session might be too basic\n",
    "            score -= 2\n",
    "        elif (session_level == \"advanced\" and experience_level == \"beginner\"):\n",
    "            # Session might be too advanced\n",
    "            score -= 3\n",
    "        \n",
    "        # Prefer upcoming sessions over past ones\n",
    "        if session.get(\"is_upcoming\", True):\n",
    "            score += 4\n",
    "        \n",
    "        # Match session category with career goals\n",
    "        session_category = session.get(\"session_category\", \"general\")\n",
    "        if any(goal in session_category for goal in career_goals) or \\\n",
    "           any(session_category in goal for goal in career_goals):\n",
    "            score += 3\n",
    "        \n",
    "        # Factor in popularity (participant count)\n",
    "        participant_count = session.get(\"participant_count\", 0)\n",
    "        if participant_count > 50:\n",
    "            score += 3\n",
    "        elif participant_count > 20:\n",
    "            score += 2\n",
    "        elif participant_count > 5:\n",
    "            score += 1\n",
    "        \n",
    "        # Video sessions (past recordings) might be immediately available\n",
    "        if session.get(\"is_video\", False):\n",
    "            score += 2\n",
    "        \n",
    "        scored_sessions.append((score, session))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_sessions.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_sessions = [session for score, session in scored_sessions[:num_recommendations]]\n",
    "    \n",
    "    return recommended_sessions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeySessionScraper(headless=False)  # Set to True for headless mode\n",
    "    sessions = scraper.scrape_sessions(max_scroll=3)\n",
    "    \n",
    "    # Save raw session data\n",
    "    scraper.save_to_json(sessions)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_sessions = scraper.process_sessions_for_recommendation(sessions)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"John Doe\",\n",
    "        \"interests\": [\"Data Analysis\", \"Machine Learning\", \"Power BI\", \"Python\"],\n",
    "        \"skills\": [\"SQL\", \"Excel\", \"Python\", \"Data Visualization\"],\n",
    "        \"career_stage\": \"mid-level\",\n",
    "        \"experience_level\": \"intermediate\",\n",
    "        \"career_goals\": [\"technical growth\", \"data science\"],\n",
    "        \"include_past_sessions\": True  # Include past sessions in recommendations\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_sessions(example_candidate, processed_sessions)\n",
    "    \n",
    "    print(f\"\\nRecommended sessions for {example_candidate['name']}:\")\n",
    "    for i, session in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {session['title']}\")\n",
    "        print(f\"   Date: {session.get('date', 'N/A')} | Time: {session.get('time', 'N/A')}\")\n",
    "        print(f\"   Host: {session.get('host', 'N/A')}\")\n",
    "        print(f\"   Type: {session.get('session_type', 'N/A')} | Category: {session.get('session_category', 'N/A')}\")\n",
    "        print(f\"   Topics: {', '.join(session.get('extracted_topics', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06a8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
