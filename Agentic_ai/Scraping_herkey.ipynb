{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da32c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded, waiting for job elements...\n",
      "Scrolling iteration 1/3...\n",
      "Scrolling iteration 2/3...\n",
      "Scrolling iteration 3/3...\n",
      "Found 60 job listings\n",
      "Saved 60 jobs to herkey_jobs.json\n",
      "\n",
      "Recommended jobs for Jane Doe:\n",
      "1. Software Engineer at Finastra (Pune | Work From Office)\n",
      "   Skills: C# • Javascript +8\n",
      "\n",
      "2. Software Engineer at Finastra (Pune | Work From Office)\n",
      "   Skills: C# • Javascript +8\n",
      "\n",
      "3. Internship | Work from Home at BTS360 (formerly GetSup... (Bangalore | Work From Home)\n",
      "   Skills: Customer Service • Sales +1\n",
      "\n",
      "4. Work from Home Internship at BTS360 (formerly GetSup... (Bangalore | Work From Home)\n",
      "   Skills: Sales • Client Acquisition\n",
      "\n",
      "5. English Trainer at FRND App (Any | Work From Home)\n",
      "   Skills: Teaching English • Communication Skill +2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class HerkeyJobScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "    def scrape_jobs(self, url: str = \"https://www.herkey.com/jobs\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape job listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more jobs)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of job dictionaries containing details of each job\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            print(\"Page loaded, waiting for job elements...\")\n",
    "            \n",
    "            # Wait for job elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='job-details']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more jobs\n",
    "            for i in range(max_scroll):\n",
    "                print(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get total count of jobs first\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='job-details']\")\n",
    "            total_jobs = len(job_elements)\n",
    "            print(f\"Found {total_jobs} job listings\")\n",
    "            \n",
    "            jobs = []\n",
    "            job_index = 0\n",
    "            \n",
    "            while job_index < total_jobs:\n",
    "                try:\n",
    "                    print(f\"Processing job {job_index + 1}/{total_jobs}...\")\n",
    "                    \n",
    "                    # Re-find all job elements to avoid stale reference\n",
    "                    job_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='job-details']\")\n",
    "                    \n",
    "                    # Check if we still have enough elements\n",
    "                    if job_index >= len(job_elements):\n",
    "                        print(f\"Not enough job elements found. Expected {job_index + 1}, found {len(job_elements)}\")\n",
    "                        break\n",
    "                    \n",
    "                    job_element = job_elements[job_index]\n",
    "                    \n",
    "                    # Extract basic job details from the current element\n",
    "                    job_data = self.extract_job_basic_info(job_element)\n",
    "                    \n",
    "                    if not job_data:\n",
    "                        job_index += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Now try to get the job URL by clicking on the job title\n",
    "                    job_url = self.get_job_url(driver, job_index, url)\n",
    "                    job_data[\"job_url\"] = job_url\n",
    "                    \n",
    "                    # Extract job ID from URL if available\n",
    "                    job_id = self.extract_job_id(job_url)\n",
    "                    job_data[\"job_id\"] = job_id\n",
    "                    \n",
    "                    jobs.append(job_data)\n",
    "                    job_index += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing job {job_index + 1}: {e}\")\n",
    "                    job_index += 1\n",
    "                    continue\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def extract_job_basic_info(self, job_element) -> Dict:\n",
    "        \"\"\"Extract basic job information from a job element\"\"\"\n",
    "        try:\n",
    "            # Extract basic job details\n",
    "            job_title = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='job-title']\").text\n",
    "            company_name = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='company-name']\").text\n",
    "            \n",
    "            # Get location, work type and experience\n",
    "            location_info = job_element.find_element(By.CSS_SELECTOR, \"p.capitalize\").text\n",
    "            \n",
    "            # Parse location info - typically in format \"City | work type | experience\"\n",
    "            location_parts = location_info.split('|')\n",
    "            location = location_parts[0].strip() if len(location_parts) > 0 else \"\"\n",
    "            work_type = location_parts[1].strip() if len(location_parts) > 1 else \"\"\n",
    "            experience = location_parts[2].strip() if len(location_parts) > 2 else \"\"\n",
    "            \n",
    "            # Try to get skills (not all jobs may have this)\n",
    "            try:\n",
    "                skills_element = job_element.find_element(By.CSS_SELECTOR, \"span.capitalize\")\n",
    "                skills = skills_element.text\n",
    "            except:\n",
    "                skills = \"\"\n",
    "            \n",
    "            # Check if the job has \"Easy Apply\" option\n",
    "            try:\n",
    "                apply_button = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='apply-job']\")\n",
    "                easy_apply = \"Easy Apply\" in apply_button.text\n",
    "            except:\n",
    "                easy_apply = False\n",
    "            \n",
    "            # Get company logo URL if available\n",
    "            try:\n",
    "                logo_element = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='company-logo'] img\")\n",
    "                company_logo = logo_element.get_attribute(\"src\")\n",
    "            except:\n",
    "                company_logo = \"\"\n",
    "            \n",
    "            return {\n",
    "                \"title\": job_title,\n",
    "                \"company\": company_name,\n",
    "                \"location\": location,\n",
    "                \"work_type\": work_type,\n",
    "                \"experience\": experience,\n",
    "                \"skills\": skills,\n",
    "                \"easy_apply\": easy_apply,\n",
    "                \"company_logo\": company_logo\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting basic job info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_job_url(self, driver, job_index: int, base_url: str) -> str:\n",
    "        \"\"\"Get job URL by clicking on job title and navigating back\"\"\"\n",
    "        job_url = \"\"\n",
    "        current_url = driver.current_url\n",
    "        \n",
    "        try:\n",
    "            # Re-find the job elements to get fresh references\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='job-details']\")\n",
    "            \n",
    "            if job_index >= len(job_elements):\n",
    "                print(f\"Job index {job_index} out of range\")\n",
    "                return \"\"\n",
    "            \n",
    "            job_element = job_elements[job_index]\n",
    "            job_title_element = job_element.find_element(By.CSS_SELECTOR, \"[data-test-id='job-title']\")\n",
    "            \n",
    "            # Click on job title to navigate to job details\n",
    "            driver.execute_script(\"arguments[0].click();\", job_title_element)\n",
    "            \n",
    "            # Wait for navigation to complete\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                lambda driver: driver.current_url != current_url\n",
    "            )\n",
    "            \n",
    "            # Get the job URL\n",
    "            job_url = driver.current_url\n",
    "            print(f\"Job URL: {job_url}\")\n",
    "            \n",
    "            # Navigate back to the job listings page\n",
    "            driver.back()\n",
    "            \n",
    "            # Wait for the job listings to load again\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='job-details']\"))\n",
    "            )\n",
    "            \n",
    "            # Additional wait to ensure page is stable\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting job URL for job {job_index + 1}: {e}\")\n",
    "            # Try to navigate back if we're stuck\n",
    "            try:\n",
    "                if driver.current_url != current_url:\n",
    "                    driver.back()\n",
    "                    WebDriverWait(driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='job-details']\"))\n",
    "                    )\n",
    "                    time.sleep(1)\n",
    "            except:\n",
    "                print(\"Failed to navigate back, continuing...\")\n",
    "        \n",
    "        return job_url\n",
    "    \n",
    "    def extract_job_id(self, job_url: str) -> str:\n",
    "        \"\"\"Extract job ID from job URL\"\"\"\n",
    "        job_id = \"\"\n",
    "        if job_url:\n",
    "            try:\n",
    "                # Extract job ID from URL (last part after the last '/')\n",
    "                job_id = job_url.split('/')[-1]\n",
    "                if not job_id.isdigit():\n",
    "                    # If last part is not a number, try to find ID in URL\n",
    "                    id_match = re.search(r'/(\\d+)/?$', job_url)\n",
    "                    if id_match:\n",
    "                        job_id = id_match.group(1)\n",
    "            except:\n",
    "                pass\n",
    "        return job_id\n",
    "    \n",
    "    def scrape_jobs_alternative_method(self, url: str = \"https://www.herkey.com/jobs\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Alternative method to scrape jobs by looking for clickable elements or href attributes\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            print(\"Page loaded, waiting for job elements...\")\n",
    "            \n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='job-details']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll to load more jobs\n",
    "            for i in range(max_scroll):\n",
    "                print(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            jobs = []\n",
    "            \n",
    "            # Try to find all clickable job elements\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='job-details']\")\n",
    "            print(f\"Found {len(job_elements)} job listings\")\n",
    "            \n",
    "            for index, job_element in enumerate(job_elements):\n",
    "                try:\n",
    "                    print(f\"Processing job {index + 1}/{len(job_elements)}...\")\n",
    "                    \n",
    "                    # Extract basic details\n",
    "                    job_data = self.extract_job_basic_info(job_element)\n",
    "                    if not job_data:\n",
    "                        continue\n",
    "                    \n",
    "                    # Look for job URL using various methods\n",
    "                    job_url = self.find_job_url_alternative(job_element, job_data[\"title\"], url)\n",
    "                    job_data[\"job_url\"] = job_url\n",
    "                    job_data[\"job_id\"] = self.extract_job_id(job_url)\n",
    "                    \n",
    "                    jobs.append(job_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting job details for job {index + 1}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def find_job_url_alternative(self, job_element, job_title: str, base_url: str) -> str:\n",
    "        \"\"\"Find job URL using alternative methods without navigation\"\"\"\n",
    "        job_url = \"\"\n",
    "        \n",
    "        # Method 1: Look for parent anchor tag\n",
    "        try:\n",
    "            parent_link = job_element.find_element(By.XPATH, \"./ancestor-or-self::a[@href]\")\n",
    "            job_url = parent_link.get_attribute(\"href\")\n",
    "            if job_url and \"/jobs/\" in job_url and job_url != base_url:\n",
    "                return job_url\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Look for any anchor tag within the job element\n",
    "        try:\n",
    "            link_elements = job_element.find_elements(By.CSS_SELECTOR, \"a[href]\")\n",
    "            for link in link_elements:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and \"/jobs/\" in href and href != base_url:\n",
    "                    job_url = href\n",
    "                    return job_url\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 3: Check for data attributes that might contain job info\n",
    "        try:\n",
    "            job_id_attr = job_element.get_attribute(\"data-job-id\")\n",
    "            if job_id_attr:\n",
    "                # Construct URL from job title and ID\n",
    "                title_slug = job_title.lower().replace(\" \", \"-\").replace(\"--\", \"-\")\n",
    "                title_slug = re.sub(r'[^a-z0-9\\-]', '', title_slug)  # Remove special chars\n",
    "                job_url = f\"https://www.herkey.com/jobs/{title_slug}/{job_id_attr}\"\n",
    "                return job_url\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 4: Look for onclick handlers or other attributes\n",
    "        try:\n",
    "            onclick = job_element.get_attribute(\"onclick\")\n",
    "            if onclick and \"/jobs/\" in onclick:\n",
    "                # Extract URL from onclick handler\n",
    "                url_match = re.search(r'[\"\\']([^\"\\']*\\/jobs\\/[^\"\\']*)[\"\\']', onclick)\n",
    "                if url_match:\n",
    "                    job_url = url_match.group(1)\n",
    "                    if not job_url.startswith(\"http\"):\n",
    "                        job_url = \"https://www.herkey.com\" + job_url\n",
    "                    return job_url\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return job_url\n",
    "    \n",
    "    def save_to_json(self, jobs: List[Dict], filename: str = \"herkey_jobs.json\"):\n",
    "        \"\"\"Save scraped jobs to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(jobs, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(jobs)} jobs to {filename}\")\n",
    "    \n",
    "    def job_recommendation_data(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process job data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes skills, experience and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_jobs = []\n",
    "        \n",
    "        for job in jobs:\n",
    "            processed_job = job.copy()\n",
    "            \n",
    "            # Extract years of experience as a numeric value\n",
    "            exp_text = job.get(\"experience\", \"\")\n",
    "            years_match = re.search(r'(\\d+)-(\\d+)\\s*Yr', exp_text)\n",
    "            if years_match:\n",
    "                min_exp = int(years_match.group(1))\n",
    "                max_exp = int(years_match.group(2))\n",
    "                processed_job[\"min_experience\"] = min_exp\n",
    "                processed_job[\"max_experience\"] = max_exp\n",
    "                processed_job[\"avg_experience\"] = (min_exp + max_exp) / 2\n",
    "            else:\n",
    "                processed_job[\"min_experience\"] = None\n",
    "                processed_job[\"max_experience\"] = None\n",
    "                processed_job[\"avg_experience\"] = None\n",
    "            \n",
    "            # Process skills into a list\n",
    "            skills_text = job.get(\"skills\", \"\")\n",
    "            # Remove the bullet points and '+n' suffix\n",
    "            skills_text = re.sub(r'\\+\\d+', '', skills_text)\n",
    "            skills_list = [skill.strip() for skill in skills_text.split('•') if skill.strip()]\n",
    "            processed_job[\"skills_list\"] = skills_list\n",
    "            \n",
    "            # Standardize work type\n",
    "            work_type = job.get(\"work_type\", \"\").lower()\n",
    "            if \"remote\" in work_type or \"from home\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"remote\"\n",
    "            elif \"office\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"in-office\"\n",
    "            elif \"hybrid\" in work_type:\n",
    "                processed_job[\"work_mode\"] = \"hybrid\"\n",
    "            else:\n",
    "                processed_job[\"work_mode\"] = \"unknown\"\n",
    "            \n",
    "            processed_jobs.append(processed_job)\n",
    "        \n",
    "        return processed_jobs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    print(\"Trying main method (clicking on job titles)...\")\n",
    "    jobs = scraper.scrape_jobs(max_scroll=3)\n",
    "    \n",
    "    # If main method doesn't work well, try alternative method\n",
    "    if not jobs or len([job for job in jobs if job.get('job_url')]) < len(jobs) * 0.5:\n",
    "        print(\"Main method didn't get enough URLs, trying alternative method...\")\n",
    "        scraper2 = HerkeyJobScraper(headless=False)\n",
    "        jobs_alt = scraper2.scrape_jobs_alternative_method(max_scroll=3)\n",
    "        \n",
    "        # Combine results, preferring jobs with URLs\n",
    "        if jobs_alt:\n",
    "            jobs = jobs_alt\n",
    "    \n",
    "    # Save raw job data\n",
    "    if jobs:\n",
    "        scraper.save_to_json(jobs)\n",
    "        print('Saved jobs to JSON')\n",
    "        \n",
    "        # Process for AI recommendations\n",
    "        processed_jobs = scraper.job_recommendation_data(jobs)\n",
    "        scraper.save_to_json(processed_jobs, \"herkey_jobs_processed.json\")\n",
    "        \n",
    "        # Print summary\n",
    "        urls_found = [job for job in jobs if job.get('job_url')]\n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"Total jobs scraped: {len(jobs)}\")\n",
    "        print(f\"Jobs with URLs: {len(urls_found)}\")\n",
    "        print(f\"Success rate: {len(urls_found)/len(jobs)*100:.1f}%\")\n",
    "        \n",
    "        if urls_found:\n",
    "            print(\"\\nSample jobs with URLs:\")\n",
    "            for i, job in enumerate(urls_found[:5]):\n",
    "                print(f\"{i+1}. {job['title']} at {job['company']}\")\n",
    "                print(f\"   URL: {job['job_url']}\")\n",
    "                print(f\"   Location: {job['location']} | {job['work_type']} | {job['experience']}\")\n",
    "                print()\n",
    "    else:\n",
    "        print(\"No jobs were scraped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e834d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:16:29,537 - __main__ - INFO - Scraping events from https://events.herkey.com/events\n",
      "c:\\Users\\knand\\OneDrive\\Desktop\\AshaAIBot\\asha\\Lib\\site-packages\\soupsieve\\css_parser.py:876: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(  # noqa: B028\n",
      "2025-04-25 08:16:31,204 - __main__ - INFO - Scraped 8 events\n",
      "2025-04-25 08:16:31,206 - __main__ - INFO - Saved 8 events to herkey_events.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended events for Jane Doe:\n",
      "1. Advance Your Career with a Premier Executive MBA from Great Lakes\n",
      "   Date: 16th Apr, 2025 to 30th Apr, 2025 | Time: 10:00am to 6:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development, event\n",
      "\n",
      "2. SkillReBoot program: Restart Your Career Journey\n",
      "   Date: 21st Apr, 2025 to 20th May, 2025 | Time: 6:00pm to 11:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "3. #STEMTheBias Scholarship - Avail Scholarships up to Rs. 80,000\n",
      "   Date: 3rd Mar, 2025 to 25th Mar, 2025 | Time: 10:00am to 11:59pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "4. LeadHERs in Tech: Tech Meets Talent , Innovation Meets Inspiration\n",
      "   Date: 21st Mar, 2025 | Time: 8:30am to 4:00pm\n",
      "   Type: Offline | Price: Free\n",
      "   Categories: Career Development, Women In Tech, networking\n",
      "\n",
      "5. herShakti\n",
      "   Date: 8th Mar, 2025 to 10th Apr, 2025 | Time: 12:00am to 11:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: JobsForHer foundation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyEventScraper:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the event scraper\"\"\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def _get_page_content(self, url: str) -> str:\n",
    "        \"\"\"Get HTML content from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def scrape_events(self, url: str = \"https://events.herkey.com/events\") -> List[Dict]:\n",
    "        \"\"\"Scrape events from HerKey events page.\"\"\"\n",
    "        logger.info(f\"Scraping events from {url}\")\n",
    "        html_content = self._get_page_content(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        events = []\n",
    "        \n",
    "        # Extract event listings based on the provided HTML structure\n",
    "        event_cards = soup.select(\".event-details-card\")\n",
    "        \n",
    "        for card in event_cards:\n",
    "            try:\n",
    "                event = {}\n",
    "                \n",
    "                # Extract event details based on the actual HTML structure\n",
    "                title_elem = card.select_one(\".card-heading\")\n",
    "                date_elem = card.select_one(\".card-body-data:has(img[src*='calendar'])\")\n",
    "                time_elem = card.select_one(\".card-body-data:has(img[src*='clock'])\")\n",
    "                location_elem = card.select_one(\".card-body-data:has(img[src*='placeholder'])\")\n",
    "                event_type_elem = card.select_one(\"span.mr-1, .card-body-data:has(i.fa-bullseye)\")\n",
    "                category_elem = card.select_one(\".card-body-data:has(img[src*='tag'])\")\n",
    "                \n",
    "                if title_elem:\n",
    "                    title_text = title_elem.get_text(strip=True)\n",
    "                    # Remove any icon text from the title\n",
    "                    if title_text:\n",
    "                        event[\"title\"] = title_text.split(\"Featured\")[0].strip()\n",
    "                \n",
    "                if date_elem:\n",
    "                    date_text = date_elem.get_text(strip=True)\n",
    "                    if date_text:\n",
    "                        event[\"date\"] = date_text\n",
    "                \n",
    "                if time_elem:\n",
    "                    time_text = time_elem.get_text(strip=True)\n",
    "                    if time_text:\n",
    "                        event[\"time\"] = time_text\n",
    "                \n",
    "                if location_elem:\n",
    "                    location_text = location_elem.get_text(strip=True)\n",
    "                    if location_text:\n",
    "                        event[\"location\"] = location_text\n",
    "                \n",
    "                if event_type_elem:\n",
    "                    event_type = event_type_elem.get_text(strip=True)\n",
    "                    if event_type and \"Online\" in event_type or \"Offline\" in event_type:\n",
    "                        event[\"event_type\"] = event_type\n",
    "                \n",
    "                if category_elem:\n",
    "                    # Parse categories (they're in a special format)\n",
    "                    category_links = category_elem.select(\"a\")\n",
    "                    if category_links:\n",
    "                        event[\"categories\"] = [link.text.strip() for link in category_links]\n",
    "                \n",
    "                # Get the registration button type\n",
    "                register_btn = card.select_one(\".register\")\n",
    "                if register_btn:\n",
    "                    event[\"registration_status\"] = \"Open\"\n",
    "                    event[\"registration_text\"] = register_btn.text.strip()\n",
    "                \n",
    "                # Check if it's a paid event\n",
    "                price_elem = card.select_one(\".card-body-data:contains('₹')\")\n",
    "                if price_elem:\n",
    "                    event[\"price\"] = price_elem.text.strip()\n",
    "                else:\n",
    "                    event[\"price\"] = \"Free\"\n",
    "                \n",
    "                # Get event ID and URL\n",
    "                event_id_span = card.select_one(\"span[id]\")\n",
    "                if event_id_span and event_id_span.get('id'):\n",
    "                    event_id = event_id_span.get('id')\n",
    "                    if \"title\" in event:\n",
    "                        event[\"url\"] = f\"{url}/{event['title'].lower().replace(' ', '-')}/{event_id}\"\n",
    "                \n",
    "                # Check if it's a featured event\n",
    "                featured_elem = card.select_one(\"img[src*='Featured'], img[width='22'][height='22']\")\n",
    "                if featured_elem:\n",
    "                    event[\"featured\"] = True\n",
    "                \n",
    "                if event:  # Only add if we found some data\n",
    "                    events.append(event)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting event data: {e}\")\n",
    "                \n",
    "        logger.info(f\"Scraped {len(events)} events\")\n",
    "        return events\n",
    "    \n",
    "    def save_to_json(self, events: List[Dict], filename: str = \"herkey_events.json\"):\n",
    "        \"\"\"Save scraped events to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(events, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(events)} events to {filename}\")\n",
    "    \n",
    "    def process_events_for_recommendation(self, events: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process event data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes categories, dates and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_events = []\n",
    "        \n",
    "        for event in events:\n",
    "            processed_event = event.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = event.get(\"date\", \"\")\n",
    "            try:\n",
    "                # Example format: \"25 Apr 2023\" - adjust pattern as needed\n",
    "                date_obj = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "                processed_event[\"datetime_obj\"] = date_obj\n",
    "                processed_event[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, keep it as is\n",
    "                processed_event[\"is_upcoming\"] = True  # Assume upcoming by default\n",
    "            \n",
    "            # Standardize event type\n",
    "            event_type = event.get(\"event_type\", \"\").lower()\n",
    "            if \"online\" in event_type:\n",
    "                processed_event[\"mode\"] = \"online\"\n",
    "            elif \"offline\" in event_type or \"in-person\" in event_type:\n",
    "                processed_event[\"mode\"] = \"offline\"\n",
    "            else:\n",
    "                processed_event[\"mode\"] = \"unknown\"\n",
    "            \n",
    "            # Standardize categories\n",
    "            categories = event.get(\"categories\", [])\n",
    "            processed_event[\"categories_lower\"] = [cat.lower() for cat in categories]\n",
    "            \n",
    "            # Determine if event is free\n",
    "            price = event.get(\"price\", \"\")\n",
    "            processed_event[\"is_free\"] = price == \"Free\" or \"free\" in price.lower()\n",
    "            \n",
    "            processed_events.append(processed_event)\n",
    "        \n",
    "        return processed_events\n",
    "\n",
    "\n",
    "def recommend_events(candidate_profile: Dict, events: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend events based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate interests, career stage, etc.\n",
    "        events: List of event dictionaries\n",
    "        num_recommendations: Number of events to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended event dictionaries\n",
    "    \"\"\"\n",
    "    # Process events for recommendation if they haven't been processed\n",
    "    if events and \"categories_lower\" not in events[0]:\n",
    "        scraper = HerkeyEventScraper()\n",
    "        events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    scored_events = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "    preferred_mode = candidate_profile.get(\"preferred_event_mode\", \"\").lower()\n",
    "    preferred_locations = [loc.lower() for loc in candidate_profile.get(\"preferred_locations\", [])]\n",
    "    career_stage = candidate_profile.get(\"career_stage\", \"\").lower()\n",
    "    \n",
    "    # Only consider upcoming events\n",
    "    upcoming_events = [event for event in events if event.get(\"is_upcoming\", True)]\n",
    "    \n",
    "    for event in upcoming_events:\n",
    "        score = 0\n",
    "        \n",
    "        # Match interests with event categories\n",
    "        event_categories = event.get(\"categories_lower\", [])\n",
    "        for interest in interests:\n",
    "            if any(interest in category for category in event_categories):\n",
    "                score += 10\n",
    "        \n",
    "        # Match event mode preference (online/offline)\n",
    "        event_mode = event.get(\"mode\", \"unknown\")\n",
    "        if preferred_mode and event_mode == preferred_mode:\n",
    "            score += 5\n",
    "        \n",
    "        # Match location preference if it's an offline event\n",
    "        if event_mode == \"offline\":\n",
    "            location = event.get(\"location\", \"\").lower()\n",
    "            if any(loc in location for loc in preferred_locations):\n",
    "                score += 5\n",
    "        \n",
    "        # Free events might be more appealing\n",
    "        if event.get(\"is_free\", False):\n",
    "            score += 2\n",
    "        \n",
    "        # Featured events might be more relevant/important\n",
    "        if event.get(\"featured\", False):\n",
    "            score += 3\n",
    "        \n",
    "        # Career stage specific events\n",
    "        event_title = event.get(\"title\", \"\").lower()\n",
    "        if career_stage in event_title or any(career_stage in cat for cat in event_categories):\n",
    "            score += 4\n",
    "        \n",
    "        scored_events.append((score, event))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_events.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_events = [event for score, event in scored_events[:num_recommendations]]\n",
    "    \n",
    "    return recommended_events\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyEventScraper()\n",
    "    events = scraper.scrape_events()\n",
    "    \n",
    "    # Save raw event data\n",
    "    scraper.save_to_json(events)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"interests\": [\"Technology\", \"Career Development\", \"Leadership\", \"AI\"],\n",
    "        \"preferred_event_mode\": \"online\",\n",
    "        \"preferred_locations\": [\"Bangalore\", \"Mumbai\", \"Delhi\"],\n",
    "        \"career_stage\": \"mid-level\"\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_events(example_candidate, processed_events)\n",
    "    \n",
    "    print(f\"\\nRecommended events for {example_candidate['name']}:\")\n",
    "    for i, event in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {event['title']}\")\n",
    "        print(f\"   Date: {event['date']} | Time: {event.get('time', 'N/A')}\")\n",
    "        print(f\"   Type: {event.get('event_type', 'N/A')} | Price: {event.get('price', 'N/A')}\")\n",
    "        print(f\"   Categories: {', '.join(event.get('categories', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:20:11,457 - __main__ - INFO - Page loaded, waiting for group elements...\n",
      "2025-04-25 08:20:12,870 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-04-25 08:20:15,220 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-04-25 08:20:17,434 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-04-25 08:20:19,658 - __main__ - INFO - Found 104 group elements\n",
      "2025-04-25 08:20:22,126 - __main__ - INFO - Saved 103 groups to herkey_groups.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended groups for Jane Doe:\n",
      "1. Women Engineers\n",
      "   Type: public | Members: 485\n",
      "   Category: development | Level: all\n",
      "   Featured: No\n",
      "\n",
      "2. Full Stack (MERN) Developer Program\n",
      "   Type: private | Members: N/A\n",
      "   Category: development | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "3. Ambassadors Club\n",
      "   Type: private | Members: 4129\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "4. Women on a Career Break\n",
      "   Type: public | Members: 36445\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "5. Women's March\n",
      "   Type: public | Members: 6066\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyGroupScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the group scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_groups(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape group listings from Herkey with URL extraction\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more groups)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of group dictionaries containing details of each group\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            # Wait for group elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='featured-group'], .MuiGrid-container.css-1d3bbye\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get count of total groups first (try multiple selectors)\n",
    "            group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye\")\n",
    "            if not group_elements:\n",
    "                group_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='featured-group']\")\n",
    "            if not group_elements:\n",
    "                group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-item\")\n",
    "            \n",
    "            total_groups = len(group_elements)\n",
    "            logger.info(f\"Found {total_groups} group listings\")\n",
    "            \n",
    "            groups = []\n",
    "            \n",
    "            # Process each group by index to avoid stale element references\n",
    "            for index in range(total_groups):\n",
    "                try:\n",
    "                    logger.info(f\"Processing group {index + 1}/{total_groups}...\")\n",
    "                    \n",
    "                    # Re-find all group elements to avoid stale reference\n",
    "                    group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye\")\n",
    "                    if not group_elements:\n",
    "                        group_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='featured-group']\")\n",
    "                    if not group_elements:\n",
    "                        group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-item\")\n",
    "                    \n",
    "                    # Check if we still have enough groups (in case page changed)\n",
    "                    if index >= len(group_elements):\n",
    "                        logger.warning(f\"Group index {index} out of range, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    group_element = group_elements[index]\n",
    "                    \n",
    "                    # Extract basic group details first\n",
    "                    group_name = \"\"\n",
    "                    group_type = \"\"\n",
    "                    member_count = 0\n",
    "                    featured = False\n",
    "                    icon_url = \"\"\n",
    "                    banner_url = \"\"\n",
    "                    category = \"\"\n",
    "                    joinable = False\n",
    "                    join_button_text = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract group name\n",
    "                        name_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-name']\")\n",
    "                        group_name = name_elem.text.strip()\n",
    "                        clickable_element = name_elem  # Use group name as clickable element\n",
    "                    except:\n",
    "                        # Try alternative selector for group name\n",
    "                        try:\n",
    "                            name_elem = group_element.find_element(By.CSS_SELECTOR, \".MuiTypography-root.MuiTypography-h6\")\n",
    "                            group_name = name_elem.text.strip()\n",
    "                            clickable_element = name_elem\n",
    "                        except:\n",
    "                            logger.warning(f\"Could not find group name for group {index + 1}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Extract other group details\n",
    "                    try:\n",
    "                        type_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-type']\")\n",
    "                        group_type = type_elem.text.strip().lower()\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        members_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-members-count']\")\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            member_count = int(members_match.group(1))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        featured_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='featured-icon']\")\n",
    "                        featured = True\n",
    "                    except:\n",
    "                        featured = False\n",
    "                    \n",
    "                    try:\n",
    "                        icon_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-icon']\")\n",
    "                        icon_url = icon_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        banner_elem = group_element.find_element(By.CSS_SELECTOR, \".css-12c20jy img\")\n",
    "                        banner_url = banner_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        category_elem = group_element.find_element(By.CSS_SELECTOR, \".MuiTypography-root.capitalize\")\n",
    "                        category = category_elem.text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        join_btn = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='join-btn'] button\")\n",
    "                        joinable = True\n",
    "                        join_button_text = join_btn.text.strip()\n",
    "                    except:\n",
    "                        joinable = False\n",
    "                    \n",
    "                    # Now get the group URL by clicking on the group name\n",
    "                    group_url = \"\"\n",
    "                    current_url = driver.current_url\n",
    "                    \n",
    "                    try:\n",
    "                        # Click on group name to navigate to group details\n",
    "                        driver.execute_script(\"arguments[0].click();\", clickable_element)\n",
    "                        \n",
    "                        # Wait a moment for navigation\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                        # Check if we're on a new page or if URL changed\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            lambda driver: driver.current_url != current_url\n",
    "                        )\n",
    "                        \n",
    "                        # Get the group URL\n",
    "                        group_url = driver.current_url\n",
    "                        logger.info(f\"Group URL: {group_url}\")\n",
    "                        \n",
    "                        # Navigate back to the groups listings page\n",
    "                        driver.back()\n",
    "                        \n",
    "                        # Wait for the group listings to load again\n",
    "                        WebDriverWait(driver, 15).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                        )\n",
    "                        \n",
    "                        # Wait for the page to be stable\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                        # Verify we're back on the main groups page\n",
    "                        if driver.current_url != url:\n",
    "                            logger.info(f\"Not on main groups page, navigating back to {url}\")\n",
    "                            driver.get(url)\n",
    "                            WebDriverWait(driver, 15).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                            )\n",
    "                            time.sleep(2)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error getting group URL for group {index + 1}: {e}\")\n",
    "                        # Try to navigate back if we're stuck\n",
    "                        try:\n",
    "                            if driver.current_url != url:\n",
    "                                logger.info(\"Attempting to navigate back to main groups page...\")\n",
    "                                driver.get(url)\n",
    "                                WebDriverWait(driver, 15).until(\n",
    "                                    EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                                )\n",
    "                                time.sleep(2)\n",
    "                        except Exception as nav_error:\n",
    "                            logger.error(f\"Error navigating back: {nav_error}\")\n",
    "                    \n",
    "                    # Extract group ID from URL if available\n",
    "                    group_id = \"\"\n",
    "                    if group_url:\n",
    "                        try:\n",
    "                            # Extract group ID from URL (last part after the last '/')\n",
    "                            group_id = group_url.split('/')[-1]\n",
    "                            if not group_id.isdigit():\n",
    "                                # If last part is not a number, try to find ID in URL\n",
    "                                id_match = re.search(r'/(\\d+)/?$', group_url)\n",
    "                                if id_match:\n",
    "                                    group_id = id_match.group(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Create group dictionary\n",
    "                    group = {\n",
    "                        \"name\": group_name,\n",
    "                        \"type\": group_type,\n",
    "                        \"member_count\": member_count,\n",
    "                        \"featured\": featured,\n",
    "                        \"icon_url\": icon_url,\n",
    "                        \"banner_url\": banner_url,\n",
    "                        \"category\": category,\n",
    "                        \"joinable\": joinable,\n",
    "                        \"join_button_text\": join_button_text,\n",
    "                        \"group_url\": group_url,\n",
    "                        \"group_id\": group_id\n",
    "                    }\n",
    "                    \n",
    "                    groups.append(group)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group details for group {index + 1}: {e}\")\n",
    "                    # Try to ensure we're on the main page before continuing\n",
    "                    try:\n",
    "                        if driver.current_url != url:\n",
    "                            driver.get(url)\n",
    "                            WebDriverWait(driver, 15).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                            )\n",
    "                            time.sleep(2)\n",
    "                    except:\n",
    "                        pass\n",
    "                    continue\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def scrape_groups_alternative_method(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Alternative method to scrape groups by looking for clickable elements or href attributes\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get page source after all content is loaded\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            \n",
    "            # Find all group container elements\n",
    "            group_elements = soup.select(\"div.MuiGrid-container.css-1d3bbye\")\n",
    "            if not group_elements:\n",
    "                group_elements = soup.select(\"[data-test-id='featured-group']\")\n",
    "            \n",
    "            logger.info(f\"Found {len(group_elements)} group elements\")\n",
    "            \n",
    "            groups = []\n",
    "            for group_element in group_elements:\n",
    "                try:\n",
    "                    group = {}\n",
    "                    \n",
    "                    # Extract group name\n",
    "                    name_elem = group_element.select_one(\"[data-test-id='group-name']\")\n",
    "                    if name_elem:\n",
    "                        group[\"name\"] = name_elem.text.strip()\n",
    "                    \n",
    "                    # Look for clickable parent element or any element that might contain href\n",
    "                    group_url = \"\"\n",
    "                    group_id = \"\"\n",
    "                    \n",
    "                    # Method 1: Look for parent anchor tag\n",
    "                    try:\n",
    "                        parent_link = group_element.find_parent(\"a\")\n",
    "                        if parent_link and parent_link.has_attr('href'):\n",
    "                            group_url = parent_link['href']\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Method 2: Look for any anchor tag within the group element\n",
    "                    if not group_url:\n",
    "                        try:\n",
    "                            link_elements = group_element.select(\"a[href]\")\n",
    "                            for link in link_elements:\n",
    "                                href = link.get('href', '')\n",
    "                                if \"/groups/\" in href and href != url:\n",
    "                                    group_url = href\n",
    "                                    break\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Method 3: Check if group element has data attributes\n",
    "                    if not group_url:\n",
    "                        try:\n",
    "                            # Look for data attributes that might contain group info\n",
    "                            group_id_attr = group_element.get('data-group-id')\n",
    "                            if group_id_attr:\n",
    "                                group_id = group_id_attr\n",
    "                                # Construct URL from group name and ID\n",
    "                                if group.get(\"name\"):\n",
    "                                    name_slug = group[\"name\"].lower().replace(\" \", \"-\").replace(\"--\", \"-\")\n",
    "                                    group_url = f\"https://www.herkey.com/groups/{name_slug}/{group_id}\"\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Extract group ID from URL if we have it\n",
    "                    if group_url and not group_id:\n",
    "                        try:\n",
    "                            group_id = group_url.split('/')[-1]\n",
    "                            if not group_id.isdigit():\n",
    "                                id_match = re.search(r'/(\\d+)/?$', group_url)\n",
    "                                if id_match:\n",
    "                                    group_id = id_match.group(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Extract other group details\n",
    "                    type_elem = group_element.select_one(\"[data-test-id='group-type']\")\n",
    "                    if type_elem:\n",
    "                        group[\"type\"] = type_elem.text.strip().lower()\n",
    "                    \n",
    "                    members_elem = group_element.select_one(\"[data-test-id='group-members-count']\")\n",
    "                    if members_elem:\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            group[\"member_count\"] = int(members_match.group(1))\n",
    "                    \n",
    "                    featured_elem = group_element.select_one(\"[data-test-id='featured-icon']\")\n",
    "                    group[\"featured\"] = featured_elem is not None\n",
    "                    \n",
    "                    icon_elem = group_element.select_one(\"[data-test-id='group-icon']\")\n",
    "                    if icon_elem and icon_elem.has_attr('src'):\n",
    "                        group[\"icon_url\"] = icon_elem['src']\n",
    "                    \n",
    "                    banner_elem = group_element.select_one(\".css-12c20jy img\")\n",
    "                    if banner_elem and banner_elem.has_attr('src'):\n",
    "                        group[\"banner_url\"] = banner_elem['src']\n",
    "                    \n",
    "                    category_elem = group_element.select_one(\".MuiTypography-root.capitalize\")\n",
    "                    if category_elem:\n",
    "                        group[\"category\"] = category_elem.text.strip()\n",
    "                    \n",
    "                    join_btn = group_element.select_one(\"[data-test-id='join-btn'] button\")\n",
    "                    if join_btn:\n",
    "                        group[\"joinable\"] = True\n",
    "                        group[\"join_button_text\"] = join_btn.text.strip()\n",
    "                    else:\n",
    "                        group[\"joinable\"] = False\n",
    "                    \n",
    "                    # Add URL and ID to group\n",
    "                    group[\"group_url\"] = group_url\n",
    "                    group[\"group_id\"] = group_id\n",
    "                    \n",
    "                    if group.get(\"name\"):  # Only add if we have at least a name\n",
    "                        groups.append(group)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group data: {e}\")\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, groups: List[Dict], filename: str = \"herkey_groups.json\"):\n",
    "        \"\"\"Save scraped groups to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(groups, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(groups)} groups to {filename}\")\n",
    "    \n",
    "    def process_groups_for_recommendation(self, groups: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process group data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes relevant fields\n",
    "        \"\"\"\n",
    "        processed_groups = []\n",
    "        \n",
    "        for group in groups:\n",
    "            processed_group = group.copy()\n",
    "            \n",
    "            # Extract topics/skills from group name where possible\n",
    "            name = group.get(\"name\", \"\").lower()\n",
    "            \n",
    "            # Extract potential skills/technologies\n",
    "            tech_keywords = [\n",
    "                \"javascript\", \"python\", \"java\", \"c++\", \"ruby\", \"php\", \"golang\", \"react\", \n",
    "                \"angular\", \"vue\", \"node\", \"express\", \"django\", \"flask\", \"spring\", \"bootstrap\",\n",
    "                \"html\", \"css\", \"sql\", \"nosql\", \"mongodb\", \"mysql\", \"postgresql\", \"oracle\",\n",
    "                \"aws\", \"azure\", \"gcp\", \"cloud\", \"devops\", \"data science\", \"machine learning\",\n",
    "                \"ai\", \"artificial intelligence\", \"blockchain\", \"iot\", \"mobile\", \"android\",\n",
    "                \"ios\", \"swift\", \"kotlin\", \"flutter\", \"react native\", \"full stack\", \"frontend\",\n",
    "                \"backend\", \"ui\", \"ux\", \"design\", \"product\", \"agile\", \"scrum\", \"kanban\",\n",
    "                \"mern\", \"mean\", \"lamp\", \"microservices\", \"docker\", \"kubernetes\", \"jenkins\",\n",
    "                \"ci/cd\", \"testing\", \"qa\", \"security\", \"cyber security\", \"data engineering\",\n",
    "                \"big data\", \"hadoop\", \"spark\", \"tableau\", \"power bi\", \"data visualization\"\n",
    "            ]\n",
    "            \n",
    "            # Look for tech keywords in the group name\n",
    "            found_keywords = [keyword for keyword in tech_keywords if keyword in name]\n",
    "            processed_group[\"tech_keywords\"] = found_keywords\n",
    "            \n",
    "            # Determine if group is for beginners, intermediate, or advanced\n",
    "            level_indicators = {\n",
    "                \"beginner\": [\"beginner\", \"basic\", \"fundamental\", \"101\", \"intro\", \"start\"],\n",
    "                \"intermediate\": [\"intermediate\", \"mid-level\"],\n",
    "                \"advanced\": [\"advanced\", \"expert\", \"professional\", \"master\", \"senior\"]\n",
    "            }\n",
    "            \n",
    "            for level, indicators in level_indicators.items():\n",
    "                if any(indicator in name for indicator in indicators):\n",
    "                    processed_group[\"level\"] = level\n",
    "                    break\n",
    "            else:\n",
    "                processed_group[\"level\"] = \"all\"  # Default if no level is detected\n",
    "            \n",
    "            # Add group category based on name (simplified)\n",
    "            if any(term in name for term in [\"developer\", \"coding\", \"programming\", \"engineer\", \"mern\", \"stack\"]):\n",
    "                processed_group[\"category\"] = \"development\"\n",
    "            elif any(term in name for term in [\"design\", \"ui\", \"ux\", \"user experience\"]):\n",
    "                processed_group[\"category\"] = \"design\"\n",
    "            elif any(term in name for term in [\"data\", \"analytics\", \"science\", \"machine learning\", \"ai\"]):\n",
    "                processed_group[\"category\"] = \"data_science\"\n",
    "            elif any(term in name for term in [\"management\", \"leader\", \"agile\", \"scrum\", \"product\"]):\n",
    "                processed_group[\"category\"] = \"management\"\n",
    "            elif any(term in name for term in [\"testing\", \"qa\", \"quality\"]):\n",
    "                processed_group[\"category\"] = \"testing\"\n",
    "            elif any(term in name for term in [\"devops\", \"cloud\", \"aws\", \"azure\", \"gcp\"]):\n",
    "                processed_group[\"category\"] = \"devops\"\n",
    "            else:\n",
    "                processed_group[\"category\"] = \"other\"\n",
    "            \n",
    "            processed_groups.append(processed_group)\n",
    "        \n",
    "        return processed_groups\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyGroupScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    logger.info(\"Trying main method (clicking on group names)...\")\n",
    "    groups = scraper.scrape_groups(max_scroll=3)\n",
    "    \n",
    "    # If main method doesn't work well, try alternative method\n",
    "    if not groups or not any(group.get('group_url') for group in groups):\n",
    "        logger.info(\"Main method didn't get URLs, trying alternative method...\")\n",
    "        scraper2 = HerkeyGroupScraper(headless=False)\n",
    "        groups = scraper2.scrape_groups_alternative_method(max_scroll=3)\n",
    "    \n",
    "    # Save raw group data\n",
    "    scraper.save_to_json(groups)\n",
    "    logger.info('Saved groups to JSON')\n",
    "    \n",
    "    # Print sample of URLs found\n",
    "    urls_found = [group for group in groups if group.get('group_url')]\n",
    "    logger.info(f\"Found URLs for {len(urls_found)} out of {len(groups)} groups\")\n",
    "    if urls_found:\n",
    "        logger.info(\"Sample URLs:\")\n",
    "        for group in urls_found[:3]:\n",
    "            logger.info(f\"- {group['name']}: {group['group_url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 08:36:58,210 - __main__ - INFO - Page loaded, waiting for session elements...\n",
      "2025-04-25 08:36:58,229 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-04-25 08:37:00,240 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-04-25 08:37:02,252 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-04-25 08:37:04,284 - __main__ - INFO - Found 29 session listings\n",
      "2025-04-25 08:37:12,997 - __main__ - INFO - Saved 29 sessions to herkey_sessions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended sessions for John Doe:\n",
      "1. Power BI Masterclass : Calculating Measures with DAX (Part 1)\n",
      "   Date: Today | Time: 10:30 AM\n",
      "   Host: Swati Agarwal\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: power bi, dax\n",
      "\n",
      "2. AccelHERate Delhi 2025 | The Future of Hiring: Merging Tech & Human Touch to Unlock Women Talent\n",
      "   Date: Today | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "3. AccelHERate Pune 2025 | Hiring Reimagined: Balancing Tech & Human Connection to Unlock Women Talent\n",
      "   Date: 16 May 25 | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "4. Paroma Chatterjee , CEO, Revolut India on transforming the Indian Fintech space.\n",
      "   Date: Today | Time: 3:00 PM\n",
      "   Host: Khushboo Ramnane\n",
      "   Type: upcoming | Category: technical\n",
      "   Topics: \n",
      "\n",
      "5. Opportunity to Network With Employers And Industry Leaders | HerRising 2025 | Bangalore\n",
      "   Date: 19 Jun 25 | Time: 10:00 AM\n",
      "   Host: Herkey Events\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeySessionScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the session scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_sessions(self, url: str = \"https://www.herkey.com/sessions\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape session listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more sessions)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of session dictionaries containing details of each session\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for session elements...\")\n",
    "            \n",
    "            # Wait for session elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[datatestid='session-card']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more sessions\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get all session cards\n",
    "            session_elements = driver.find_elements(By.CSS_SELECTOR, \"[datatestid='session-card'], [data-test-id='session-card']\")\n",
    "            logger.info(f\"Found {len(session_elements)} session listings\")\n",
    "            \n",
    "            sessions = []\n",
    "            for session_element in session_elements:\n",
    "                try:\n",
    "                    session = {}\n",
    "                    \n",
    "                    # Extract session time, date\n",
    "                    try:\n",
    "                        time_data = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='time-data']\").text\n",
    "                        if time_data:\n",
    "                            # Parse date and time (format: \"24 Apr 25 | 10:30 AM\")\n",
    "                            date_time_parts = time_data.split('|')\n",
    "                            if len(date_time_parts) > 0:\n",
    "                                session[\"date\"] = date_time_parts[0].strip()\n",
    "                            if len(date_time_parts) > 1:\n",
    "                                session[\"time\"] = date_time_parts[1].strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract time data\")\n",
    "                    \n",
    "                    # Extract session title\n",
    "                    try:\n",
    "                        discussion_subject = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-subject'] p\").text\n",
    "                        if discussion_subject:\n",
    "                            session[\"title\"] = discussion_subject\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session title\")\n",
    "                    \n",
    "                    # Extract host info\n",
    "                    try:\n",
    "                        host_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='nav-to-user-profile'] h6\")\n",
    "                        if host_element:\n",
    "                            session[\"host\"] = host_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host name\")\n",
    "                    \n",
    "                    # Extract host headline/role if available\n",
    "                    try:\n",
    "                        headline_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='headline']\")\n",
    "                        if headline_element and headline_element.text.strip():\n",
    "                            session[\"host_headline\"] = headline_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host headline\")\n",
    "                    \n",
    "                    # Get host stage/level\n",
    "                    try:\n",
    "                        stage_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-stage'] span\")\n",
    "                        if stage_element:\n",
    "                            session[\"host_stage\"] = stage_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host stage\")\n",
    "                    \n",
    "                    # Get participant count\n",
    "                    try:\n",
    "                        participant_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-pic']\")\n",
    "                        participant_text = participant_element.text\n",
    "                        if participant_text and participant_text.startswith(\"+\"):\n",
    "                            session[\"participant_count\"] = int(participant_text.replace(\"+\", \"\").strip())\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract participant count\")\n",
    "                    \n",
    "                    # Get session type (past/upcoming)\n",
    "                    try:\n",
    "                        session_type = session_element.get_attribute(\"data-sessiontype\")\n",
    "                        if session_type:\n",
    "                            session[\"session_type\"] = session_type\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session type\")\n",
    "                    \n",
    "                    # Get session ID\n",
    "                    try:\n",
    "                        session_id = session_element.get_attribute(\"data-id\")\n",
    "                        if session_id:\n",
    "                            session[\"id\"] = session_id\n",
    "                            # Construct URL\n",
    "                            if \"title\" in session:\n",
    "                                slug = session[\"title\"].lower().replace(\" \", \"-\")\n",
    "                                session[\"url\"] = f\"{url}/{slug}/{session_id}\"\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session ID\")\n",
    "                    \n",
    "                    # Check if it's a video session\n",
    "                    try:\n",
    "                        video_element = session_element.find_element(By.CSS_SELECTOR, \"div[style*='youtube.com']\")\n",
    "                        if video_element:\n",
    "                            session[\"is_video\"] = True\n",
    "                            video_url = video_element.get_attribute(\"style\")\n",
    "                            # Extract YouTube video ID\n",
    "                            youtube_match = re.search(r'youtube.com/vi/([^/]+)/', video_url)\n",
    "                            if youtube_match:\n",
    "                                session[\"youtube_id\"] = youtube_match.group(1)\n",
    "                    except:\n",
    "                        session[\"is_video\"] = False\n",
    "                    \n",
    "                    # Get session status/action button text\n",
    "                    try:\n",
    "                        status_button = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-status-button'] button p\")\n",
    "                        if status_button:\n",
    "                            session[\"action\"] = status_button.text.strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract action button text\")\n",
    "                    \n",
    "                    if session:  # Only add if we found some data\n",
    "                        sessions.append(session)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting session data: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return sessions\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, sessions: List[Dict], filename: str = \"herkey_sessions.json\"):\n",
    "        \"\"\"Save scraped sessions to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sessions, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(sessions)} sessions to {filename}\")\n",
    "    \n",
    "    def process_sessions_for_recommendation(self, sessions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process session data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes dates, topics and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_sessions = []\n",
    "        \n",
    "        for session in sessions:\n",
    "            processed_session = session.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = session.get(\"date\", \"\")\n",
    "            time_str = session.get(\"time\", \"\")\n",
    "            datetime_str = f\"{date_str} {time_str}\".strip()\n",
    "\n",
    "            # In the process_sessions_for_recommendation method\n",
    "            try:\n",
    "                # Try various date formats\n",
    "                date_obj = None  # Initialize date_obj to avoid the UnboundLocalError\n",
    "                \n",
    "                if len(date_str.split()) == 3:  # Format: \"24 Apr 25\"\n",
    "                    date_obj = datetime.strptime(datetime_str, \"%d %b %y %I:%M %p\")\n",
    "                else:\n",
    "                    # Try other formats if the first one fails\n",
    "                    date_formats = [\n",
    "                        \"%d %b %Y %I:%M %p\",  # 24 Apr 2025 10:30 AM\n",
    "                        \"%d %B %Y %I:%M %p\",  # 24 April 2025 10:30 AM\n",
    "                        \"%b %d, %Y %I:%M %p\"  # Apr 24, 2025 10:30 AM\n",
    "                    ]\n",
    "                    \n",
    "                    for date_format in date_formats:\n",
    "                        try:\n",
    "                            date_obj = datetime.strptime(datetime_str, date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                # Check if date_obj was successfully set\n",
    "                if date_obj:\n",
    "                    processed_session[\"datetime_obj\"] = date_obj\n",
    "                    processed_session[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "                else:\n",
    "                    # Handle case where no format worked\n",
    "                    raise ValueError(\"Could not parse date string\")\n",
    "                    \n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, determine upcoming from session_type\n",
    "                if session.get(\"session_type\") == \"upcoming\":\n",
    "                    processed_session[\"is_upcoming\"] = True\n",
    "                else:\n",
    "                    processed_session[\"is_upcoming\"] = False\n",
    "            \n",
    "            # Extract topics from title\n",
    "            title = session.get(\"title\", \"\").lower()\n",
    "            topics = []\n",
    "            \n",
    "            # Common tech topics to check in the title\n",
    "            tech_topics = [\n",
    "                \"python\", \"javascript\", \"react\", \"angular\", \"vue\", \"node\", \"java\", \"c++\", \"c#\", \n",
    "                \"php\", \"ruby\", \"golang\", \"rust\", \"swift\", \"kotlin\", \"sql\", \"nosql\", \"mongodb\",\n",
    "                \"database\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"devops\",\n",
    "                \"ai\", \"machine learning\", \"data science\", \"deep learning\", \"nlp\", \"computer vision\",\n",
    "                \"blockchain\", \"iot\", \"mobile\", \"web\", \"frontend\", \"backend\", \"fullstack\", \"ui\", \"ux\",\n",
    "                \"testing\", \"qa\", \"security\", \"agile\", \"scrum\", \"kanban\", \"product management\",\n",
    "                \"power bi\", \"tableau\", \"excel\", \"analytics\", \"big data\", \"hadoop\", \"spark\", \n",
    "                \"cybersecurity\", \"networking\", \"seo\", \"digital marketing\", \"dax\"\n",
    "            ]\n",
    "            \n",
    "            for topic in tech_topics:\n",
    "                if topic in title:\n",
    "                    topics.append(topic)\n",
    "            \n",
    "            processed_session[\"extracted_topics\"] = topics\n",
    "            \n",
    "            # Determine if session is technical or soft-skills\n",
    "            tech_indicators = [\"programming\", \"code\", \"developer\", \"software\", \"tech\", \"data\", \n",
    "                               \"engineering\", \"algorithm\", \"system\", \"database\", \"cloud\", \"devops\"]\n",
    "            \n",
    "            softskill_indicators = [\"career\", \"leadership\", \"management\", \"communication\",\n",
    "                                   \"soft skill\", \"interview\", \"resume\", \"cv\", \"personal\",\n",
    "                                   \"growth\", \"mindset\", \"wellbeing\", \"mental health\"]\n",
    "            \n",
    "            tech_score = sum(1 for indicator in tech_indicators if indicator in title)\n",
    "            softskill_score = sum(1 for indicator in softskill_indicators if indicator in title)\n",
    "            \n",
    "            if tech_score > softskill_score:\n",
    "                processed_session[\"session_category\"] = \"technical\"\n",
    "            elif softskill_score > tech_score:\n",
    "                processed_session[\"session_category\"] = \"soft skills\"\n",
    "            else:\n",
    "                processed_session[\"session_category\"] = \"general\"\n",
    "            \n",
    "            # Determine experience level based on title\n",
    "            if any(x in title for x in [\"beginner\", \"basic\", \"introduction\", \"101\", \"fundamentals\"]):\n",
    "                processed_session[\"experience_level\"] = \"beginner\"\n",
    "            elif any(x in title for x in [\"advanced\", \"expert\", \"mastery\", \"professional\"]):\n",
    "                processed_session[\"experience_level\"] = \"advanced\"\n",
    "            elif any(x in title for x in [\"intermediate\", \"part 2\", \"level 2\"]):\n",
    "                processed_session[\"experience_level\"] = \"intermediate\"\n",
    "            else:\n",
    "                processed_session[\"experience_level\"] = \"all levels\"\n",
    "            \n",
    "            processed_sessions.append(processed_session)\n",
    "        \n",
    "        return processed_sessions\n",
    "\n",
    "\n",
    "def recommend_sessions(candidate_profile: Dict, sessions: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend sessions based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate interests, experience level, etc.\n",
    "        sessions: List of session dictionaries\n",
    "        num_recommendations: Number of sessions to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended session dictionaries\n",
    "    \"\"\"\n",
    "    # Process sessions for recommendation if they haven't been processed\n",
    "    if sessions and \"extracted_topics\" not in sessions[0]:\n",
    "        scraper = HerkeySessionScraper()\n",
    "        sessions = scraper.process_sessions_for_recommendation(sessions)\n",
    "    \n",
    "    scored_sessions = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "    skills = [skill.lower() for skill in candidate_profile.get(\"skills\", [])]\n",
    "    career_stage = candidate_profile.get(\"career_stage\", \"\").lower()\n",
    "    experience_level = candidate_profile.get(\"experience_level\", \"intermediate\").lower()\n",
    "    career_goals = [goal.lower() for goal in candidate_profile.get(\"career_goals\", [])]\n",
    "    \n",
    "    # Combine interests and skills for broader matching\n",
    "    all_interests = interests + skills\n",
    "    \n",
    "    # Only consider upcoming sessions by default unless specified otherwise\n",
    "    if candidate_profile.get(\"include_past_sessions\", False):\n",
    "        filtered_sessions = sessions\n",
    "    else:\n",
    "        filtered_sessions = [session for session in sessions if session.get(\"is_upcoming\", True)]\n",
    "    \n",
    "    for session in filtered_sessions:\n",
    "        score = 0\n",
    "        \n",
    "        # Match extracted topics with interests and skills\n",
    "        for topic in session.get(\"extracted_topics\", []):\n",
    "            if any(interest in topic or topic in interest for interest in all_interests):\n",
    "                score += 10\n",
    "        \n",
    "        # Match session title keywords with interests and skills\n",
    "        title = session.get(\"title\", \"\").lower()\n",
    "        for interest in all_interests:\n",
    "            if interest in title:\n",
    "                score += 5\n",
    "        \n",
    "        # Match experience level\n",
    "        session_level = session.get(\"experience_level\", \"all levels\")\n",
    "        if session_level == \"all levels\" or session_level == experience_level:\n",
    "            score += 3\n",
    "        elif (session_level == \"beginner\" and experience_level in [\"intermediate\", \"advanced\"]) or \\\n",
    "             (session_level == \"intermediate\" and experience_level == \"advanced\"):\n",
    "            # Session might be too basic\n",
    "            score -= 2\n",
    "        elif (session_level == \"advanced\" and experience_level == \"beginner\"):\n",
    "            # Session might be too advanced\n",
    "            score -= 3\n",
    "        \n",
    "        # Prefer upcoming sessions over past ones\n",
    "        if session.get(\"is_upcoming\", True):\n",
    "            score += 4\n",
    "        \n",
    "        # Match session category with career goals\n",
    "        session_category = session.get(\"session_category\", \"general\")\n",
    "        if any(goal in session_category for goal in career_goals) or \\\n",
    "           any(session_category in goal for goal in career_goals):\n",
    "            score += 3\n",
    "        \n",
    "        # Factor in popularity (participant count)\n",
    "        participant_count = session.get(\"participant_count\", 0)\n",
    "        if participant_count > 50:\n",
    "            score += 3\n",
    "        elif participant_count > 20:\n",
    "            score += 2\n",
    "        elif participant_count > 5:\n",
    "            score += 1\n",
    "        \n",
    "        # Video sessions (past recordings) might be immediately available\n",
    "        if session.get(\"is_video\", False):\n",
    "            score += 2\n",
    "        \n",
    "        scored_sessions.append((score, session))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_sessions.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_sessions = [session for score, session in scored_sessions[:num_recommendations]]\n",
    "    \n",
    "    return recommended_sessions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeySessionScraper(headless=False)  # Set to True for headless mode\n",
    "    sessions = scraper.scrape_sessions(max_scroll=3)\n",
    "    \n",
    "    # Save raw session data\n",
    "    scraper.save_to_json(sessions)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_sessions = scraper.process_sessions_for_recommendation(sessions)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"John Doe\",\n",
    "        \"interests\": [\"Data Analysis\", \"Machine Learning\", \"Power BI\", \"Python\"],\n",
    "        \"skills\": [\"SQL\", \"Excel\", \"Python\", \"Data Visualization\"],\n",
    "        \"career_stage\": \"mid-level\",\n",
    "        \"experience_level\": \"intermediate\",\n",
    "        \"career_goals\": [\"technical growth\", \"data science\"],\n",
    "        \"include_past_sessions\": True  # Include past sessions in recommendations\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_sessions(example_candidate, processed_sessions)\n",
    "    \n",
    "    print(f\"\\nRecommended sessions for {example_candidate['name']}:\")\n",
    "    for i, session in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {session['title']}\")\n",
    "        print(f\"   Date: {session.get('date', 'N/A')} | Time: {session.get('time', 'N/A')}\")\n",
    "        print(f\"   Host: {session.get('host', 'N/A')}\")\n",
    "        print(f\"   Type: {session.get('session_type', 'N/A')} | Category: {session.get('session_category', 'N/A')}\")\n",
    "        print(f\"   Topics: {', '.join(session.get('extracted_topics', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06a8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
