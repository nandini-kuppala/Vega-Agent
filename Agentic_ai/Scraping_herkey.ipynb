{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:38:17,504 - __main__ - INFO - Scraping events from https://events.herkey.com/events\n",
      "c:\\Users\\knand\\OneDrive\\Desktop\\AshaAIBot\\asha\\Lib\\site-packages\\soupsieve\\css_parser.py:876: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(  # noqa: B028\n",
      "2025-05-19 22:38:19,533 - __main__ - INFO - Scraped 8 events\n",
      "2025-05-19 22:38:19,535 - __main__ - INFO - Saved 8 events to herkey_events.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended events for Jane Doe:\n",
      "1. Advance Your Career with a Premier Executive MBA from Great Lakes\n",
      "   Date: 16th Apr, 2025 to 31st May, 2025 | Time: 10:00am to 6:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development, event\n",
      "\n",
      "2. #STEMTheBias Scholarship - Avail Scholarships up to Rs. 80,000\n",
      "   Date: 3rd Mar, 2025 to 25th Mar, 2025 | Time: 10:00am to 11:59pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "3. LeadHERs in Tech: Tech Meets Talent , Innovation Meets Inspiration\n",
      "   Date: 21st Mar, 2025 | Time: 8:30am to 4:00pm\n",
      "   Type: Offline | Price: Free\n",
      "   Categories: Career Development, Women In Tech, networking\n",
      "\n",
      "4. SkillReBoot program: Restart Your Career Journey\n",
      "   Date: 21st Apr, 2025 to 20th May, 2025 | Time: 6:00pm to 11:00pm\n",
      "   Type: Online | Price: ₹ 1\n",
      "₹ 30000\n",
      "   Categories: Career Development\n",
      "\n",
      "5. HerFreshStart: Scholarships of up to 75% for Mothers Returning to Work\n",
      "   Date: 8th May, 2025 to 25th May, 2025 | Time: 3:43pm to 6:00pm\n",
      "   Type: Offline | Price: Free\n",
      "   Categories: Career Development, Diversity Drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyGroupScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the group scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_groups(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape group listings from Herkey with URL extraction\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more groups)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of group dictionaries containing details of each group\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            # Wait for group elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='featured-group'], .MuiGrid-container.css-1d3bbye\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get count of total groups first (try multiple selectors)\n",
    "            group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye\")\n",
    "            if not group_elements:\n",
    "                group_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='featured-group']\")\n",
    "            if not group_elements:\n",
    "                group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-item\")\n",
    "            \n",
    "            total_groups = len(group_elements)\n",
    "            logger.info(f\"Found {total_groups} group listings\")\n",
    "            \n",
    "            groups = []\n",
    "            \n",
    "            # Process each group by index to avoid stale element references\n",
    "            for index in range(total_groups):\n",
    "                try:\n",
    "                    logger.info(f\"Processing group {index + 1}/{total_groups}...\")\n",
    "                    \n",
    "                    # Re-find all group elements to avoid stale reference\n",
    "                    group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye\")\n",
    "                    if not group_elements:\n",
    "                        group_elements = driver.find_elements(By.CSS_SELECTOR, \"[data-test-id='featured-group']\")\n",
    "                    if not group_elements:\n",
    "                        group_elements = driver.find_elements(By.CSS_SELECTOR, \".MuiGrid-item\")\n",
    "                    \n",
    "                    # Check if we still have enough groups (in case page changed)\n",
    "                    if index >= len(group_elements):\n",
    "                        logger.warning(f\"Group index {index} out of range, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    group_element = group_elements[index]\n",
    "                    \n",
    "                    # Extract basic group details first\n",
    "                    group_name = \"\"\n",
    "                    group_type = \"\"\n",
    "                    member_count = 0\n",
    "                    featured = False\n",
    "                    icon_url = \"\"\n",
    "                    banner_url = \"\"\n",
    "                    category = \"\"\n",
    "                    joinable = False\n",
    "                    join_button_text = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract group name\n",
    "                        name_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-name']\")\n",
    "                        group_name = name_elem.text.strip()\n",
    "                        clickable_element = name_elem  # Use group name as clickable element\n",
    "                    except:\n",
    "                        # Try alternative selector for group name\n",
    "                        try:\n",
    "                            name_elem = group_element.find_element(By.CSS_SELECTOR, \".MuiTypography-root.MuiTypography-h6\")\n",
    "                            group_name = name_elem.text.strip()\n",
    "                            clickable_element = name_elem\n",
    "                        except:\n",
    "                            logger.warning(f\"Could not find group name for group {index + 1}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Extract other group details\n",
    "                    try:\n",
    "                        type_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-type']\")\n",
    "                        group_type = type_elem.text.strip().lower()\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        members_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-members-count']\")\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            member_count = int(members_match.group(1))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        featured_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='featured-icon']\")\n",
    "                        featured = True\n",
    "                    except:\n",
    "                        featured = False\n",
    "                    \n",
    "                    try:\n",
    "                        icon_elem = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='group-icon']\")\n",
    "                        icon_url = icon_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        banner_elem = group_element.find_element(By.CSS_SELECTOR, \".css-12c20jy img\")\n",
    "                        banner_url = banner_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        category_elem = group_element.find_element(By.CSS_SELECTOR, \".MuiTypography-root.capitalize\")\n",
    "                        category = category_elem.text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        join_btn = group_element.find_element(By.CSS_SELECTOR, \"[data-test-id='join-btn'] button\")\n",
    "                        joinable = True\n",
    "                        join_button_text = join_btn.text.strip()\n",
    "                    except:\n",
    "                        joinable = False\n",
    "                    \n",
    "                    # Now get the group URL by clicking on the group name\n",
    "                    group_url = \"\"\n",
    "                    current_url = driver.current_url\n",
    "                    \n",
    "                    try:\n",
    "                        # Click on group name to navigate to group details\n",
    "                        driver.execute_script(\"arguments[0].click();\", clickable_element)\n",
    "                        \n",
    "                        # Wait a moment for navigation\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                        # Check if we're on a new page or if URL changed\n",
    "                        WebDriverWait(driver, 10).until(\n",
    "                            lambda driver: driver.current_url != current_url\n",
    "                        )\n",
    "                        \n",
    "                        # Get the group URL\n",
    "                        group_url = driver.current_url\n",
    "                        logger.info(f\"Group URL: {group_url}\")\n",
    "                        \n",
    "                        # Navigate back to the groups listings page\n",
    "                        driver.back()\n",
    "                        \n",
    "                        # Wait for the group listings to load again\n",
    "                        WebDriverWait(driver, 15).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                        )\n",
    "                        \n",
    "                        # Wait for the page to be stable\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                        # Verify we're back on the main groups page\n",
    "                        if driver.current_url != url:\n",
    "                            logger.info(f\"Not on main groups page, navigating back to {url}\")\n",
    "                            driver.get(url)\n",
    "                            WebDriverWait(driver, 15).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                            )\n",
    "                            time.sleep(2)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error getting group URL for group {index + 1}: {e}\")\n",
    "                        # Try to navigate back if we're stuck\n",
    "                        try:\n",
    "                            if driver.current_url != url:\n",
    "                                logger.info(\"Attempting to navigate back to main groups page...\")\n",
    "                                driver.get(url)\n",
    "                                WebDriverWait(driver, 15).until(\n",
    "                                    EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                                )\n",
    "                                time.sleep(2)\n",
    "                        except Exception as nav_error:\n",
    "                            logger.error(f\"Error navigating back: {nav_error}\")\n",
    "                    \n",
    "                    # Extract group ID from URL if available\n",
    "                    group_id = \"\"\n",
    "                    if group_url:\n",
    "                        try:\n",
    "                            # Extract group ID from URL (last part after the last '/')\n",
    "                            group_id = group_url.split('/')[-1]\n",
    "                            if not group_id.isdigit():\n",
    "                                # If last part is not a number, try to find ID in URL\n",
    "                                id_match = re.search(r'/(\\d+)/?$', group_url)\n",
    "                                if id_match:\n",
    "                                    group_id = id_match.group(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Create group dictionary\n",
    "                    group = {\n",
    "                        \"name\": group_name,\n",
    "                        \"type\": group_type,\n",
    "                        \"member_count\": member_count,\n",
    "                        \"featured\": featured,\n",
    "                        \"icon_url\": icon_url,\n",
    "                        \"banner_url\": banner_url,\n",
    "                        \"category\": category,\n",
    "                        \"joinable\": joinable,\n",
    "                        \"join_button_text\": join_button_text,\n",
    "                        \"group_url\": group_url,\n",
    "                        \"group_id\": group_id\n",
    "                    }\n",
    "                    \n",
    "                    groups.append(group)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group details for group {index + 1}: {e}\")\n",
    "                    # Try to ensure we're on the main page before continuing\n",
    "                    try:\n",
    "                        if driver.current_url != url:\n",
    "                            driver.get(url)\n",
    "                            WebDriverWait(driver, 15).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group'], .MuiGrid-item\"))\n",
    "                            )\n",
    "                            time.sleep(2)\n",
    "                    except:\n",
    "                        pass\n",
    "                    continue\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def scrape_groups_alternative_method(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Alternative method to scrape groups by looking for clickable elements or href attributes\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".MuiGrid-container.css-1d3bbye, [data-test-id='featured-group']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get page source after all content is loaded\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            \n",
    "            # Find all group container elements\n",
    "            group_elements = soup.select(\"div.MuiGrid-container.css-1d3bbye\")\n",
    "            if not group_elements:\n",
    "                group_elements = soup.select(\"[data-test-id='featured-group']\")\n",
    "            \n",
    "            logger.info(f\"Found {len(group_elements)} group elements\")\n",
    "            \n",
    "            groups = []\n",
    "            for group_element in group_elements:\n",
    "                try:\n",
    "                    group = {}\n",
    "                    \n",
    "                    # Extract group name\n",
    "                    name_elem = group_element.select_one(\"[data-test-id='group-name']\")\n",
    "                    if name_elem:\n",
    "                        group[\"name\"] = name_elem.text.strip()\n",
    "                    \n",
    "                    # Look for clickable parent element or any element that might contain href\n",
    "                    group_url = \"\"\n",
    "                    group_id = \"\"\n",
    "                    \n",
    "                    # Method 1: Look for parent anchor tag\n",
    "                    try:\n",
    "                        parent_link = group_element.find_parent(\"a\")\n",
    "                        if parent_link and parent_link.has_attr('href'):\n",
    "                            group_url = parent_link['href']\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Method 2: Look for any anchor tag within the group element\n",
    "                    if not group_url:\n",
    "                        try:\n",
    "                            link_elements = group_element.select(\"a[href]\")\n",
    "                            for link in link_elements:\n",
    "                                href = link.get('href', '')\n",
    "                                if \"/groups/\" in href and href != url:\n",
    "                                    group_url = href\n",
    "                                    break\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Method 3: Check if group element has data attributes\n",
    "                    if not group_url:\n",
    "                        try:\n",
    "                            # Look for data attributes that might contain group info\n",
    "                            group_id_attr = group_element.get('data-group-id')\n",
    "                            if group_id_attr:\n",
    "                                group_id = group_id_attr\n",
    "                                # Construct URL from group name and ID\n",
    "                                if group.get(\"name\"):\n",
    "                                    name_slug = group[\"name\"].lower().replace(\" \", \"-\").replace(\"--\", \"-\")\n",
    "                                    group_url = f\"https://www.herkey.com/groups/{name_slug}/{group_id}\"\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Extract group ID from URL if we have it\n",
    "                    if group_url and not group_id:\n",
    "                        try:\n",
    "                            group_id = group_url.split('/')[-1]\n",
    "                            if not group_id.isdigit():\n",
    "                                id_match = re.search(r'/(\\d+)/?$', group_url)\n",
    "                                if id_match:\n",
    "                                    group_id = id_match.group(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Extract other group details\n",
    "                    type_elem = group_element.select_one(\"[data-test-id='group-type']\")\n",
    "                    if type_elem:\n",
    "                        group[\"type\"] = type_elem.text.strip().lower()\n",
    "                    \n",
    "                    members_elem = group_element.select_one(\"[data-test-id='group-members-count']\")\n",
    "                    if members_elem:\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            group[\"member_count\"] = int(members_match.group(1))\n",
    "                    \n",
    "                    featured_elem = group_element.select_one(\"[data-test-id='featured-icon']\")\n",
    "                    group[\"featured\"] = featured_elem is not None\n",
    "                    \n",
    "                    icon_elem = group_element.select_one(\"[data-test-id='group-icon']\")\n",
    "                    if icon_elem and icon_elem.has_attr('src'):\n",
    "                        group[\"icon_url\"] = icon_elem['src']\n",
    "                    \n",
    "                    banner_elem = group_element.select_one(\".css-12c20jy img\")\n",
    "                    if banner_elem and banner_elem.has_attr('src'):\n",
    "                        group[\"banner_url\"] = banner_elem['src']\n",
    "                    \n",
    "                    category_elem = group_element.select_one(\".MuiTypography-root.capitalize\")\n",
    "                    if category_elem:\n",
    "                        group[\"category\"] = category_elem.text.strip()\n",
    "                    \n",
    "                    join_btn = group_element.select_one(\"[data-test-id='join-btn'] button\")\n",
    "                    if join_btn:\n",
    "                        group[\"joinable\"] = True\n",
    "                        group[\"join_button_text\"] = join_btn.text.strip()\n",
    "                    else:\n",
    "                        group[\"joinable\"] = False\n",
    "                    \n",
    "                    # Add URL and ID to group\n",
    "                    group[\"group_url\"] = group_url\n",
    "                    group[\"group_id\"] = group_id\n",
    "                    \n",
    "                    if group.get(\"name\"):  # Only add if we have at least a name\n",
    "                        groups.append(group)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group data: {e}\")\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, groups: List[Dict], filename: str = \"herkey_groups.json\"):\n",
    "        \"\"\"Save scraped groups to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(groups, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(groups)} groups to {filename}\")\n",
    "    \n",
    "    def process_groups_for_recommendation(self, groups: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process group data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes relevant fields\n",
    "        \"\"\"\n",
    "        processed_groups = []\n",
    "        \n",
    "        for group in groups:\n",
    "            processed_group = group.copy()\n",
    "            \n",
    "            # Extract topics/skills from group name where possible\n",
    "            name = group.get(\"name\", \"\").lower()\n",
    "            \n",
    "            # Extract potential skills/technologies\n",
    "            tech_keywords = [\n",
    "                \"javascript\", \"python\", \"java\", \"c++\", \"ruby\", \"php\", \"golang\", \"react\", \n",
    "                \"angular\", \"vue\", \"node\", \"express\", \"django\", \"flask\", \"spring\", \"bootstrap\",\n",
    "                \"html\", \"css\", \"sql\", \"nosql\", \"mongodb\", \"mysql\", \"postgresql\", \"oracle\",\n",
    "                \"aws\", \"azure\", \"gcp\", \"cloud\", \"devops\", \"data science\", \"machine learning\",\n",
    "                \"ai\", \"artificial intelligence\", \"blockchain\", \"iot\", \"mobile\", \"android\",\n",
    "                \"ios\", \"swift\", \"kotlin\", \"flutter\", \"react native\", \"full stack\", \"frontend\",\n",
    "                \"backend\", \"ui\", \"ux\", \"design\", \"product\", \"agile\", \"scrum\", \"kanban\",\n",
    "                \"mern\", \"mean\", \"lamp\", \"microservices\", \"docker\", \"kubernetes\", \"jenkins\",\n",
    "                \"ci/cd\", \"testing\", \"qa\", \"security\", \"cyber security\", \"data engineering\",\n",
    "                \"big data\", \"hadoop\", \"spark\", \"tableau\", \"power bi\", \"data visualization\"\n",
    "            ]\n",
    "            \n",
    "            # Look for tech keywords in the group name\n",
    "            found_keywords = [keyword for keyword in tech_keywords if keyword in name]\n",
    "            processed_group[\"tech_keywords\"] = found_keywords\n",
    "            \n",
    "            # Determine if group is for beginners, intermediate, or advanced\n",
    "            level_indicators = {\n",
    "                \"beginner\": [\"beginner\", \"basic\", \"fundamental\", \"101\", \"intro\", \"start\"],\n",
    "                \"intermediate\": [\"intermediate\", \"mid-level\"],\n",
    "                \"advanced\": [\"advanced\", \"expert\", \"professional\", \"master\", \"senior\"]\n",
    "            }\n",
    "            \n",
    "            for level, indicators in level_indicators.items():\n",
    "                if any(indicator in name for indicator in indicators):\n",
    "                    processed_group[\"level\"] = level\n",
    "                    break\n",
    "            else:\n",
    "                processed_group[\"level\"] = \"all\"  # Default if no level is detected\n",
    "            \n",
    "            # Add group category based on name (simplified)\n",
    "            if any(term in name for term in [\"developer\", \"coding\", \"programming\", \"engineer\", \"mern\", \"stack\"]):\n",
    "                processed_group[\"category\"] = \"development\"\n",
    "            elif any(term in name for term in [\"design\", \"ui\", \"ux\", \"user experience\"]):\n",
    "                processed_group[\"category\"] = \"design\"\n",
    "            elif any(term in name for term in [\"data\", \"analytics\", \"science\", \"machine learning\", \"ai\"]):\n",
    "                processed_group[\"category\"] = \"data_science\"\n",
    "            elif any(term in name for term in [\"management\", \"leader\", \"agile\", \"scrum\", \"product\"]):\n",
    "                processed_group[\"category\"] = \"management\"\n",
    "            elif any(term in name for term in [\"testing\", \"qa\", \"quality\"]):\n",
    "                processed_group[\"category\"] = \"testing\"\n",
    "            elif any(term in name for term in [\"devops\", \"cloud\", \"aws\", \"azure\", \"gcp\"]):\n",
    "                processed_group[\"category\"] = \"devops\"\n",
    "            else:\n",
    "                processed_group[\"category\"] = \"other\"\n",
    "            \n",
    "            processed_groups.append(processed_group)\n",
    "        \n",
    "        return processed_groups\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyGroupScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    logger.info(\"Trying main method (clicking on group names)...\")\n",
    "    groups = scraper.scrape_groups(max_scroll=3)\n",
    "    \n",
    "    # If main method doesn't work well, try alternative method\n",
    "    if not groups or not any(group.get('group_url') for group in groups):\n",
    "        logger.info(\"Main method didn't get URLs, trying alternative method...\")\n",
    "        scraper2 = HerkeyGroupScraper(headless=False)\n",
    "        groups = scraper2.scrape_groups_alternative_method(max_scroll=3)\n",
    "    \n",
    "    # Save raw group data\n",
    "    scraper.save_to_json(groups)\n",
    "    logger.info('Saved groups to JSON')\n",
    "    \n",
    "    # Print sample of URLs found\n",
    "    urls_found = [group for group in groups if group.get('group_url')]\n",
    "    logger.info(f\"Found URLs for {len(urls_found)} out of {len(groups)} groups\")\n",
    "    if urls_found:\n",
    "        logger.info(\"Sample URLs:\")\n",
    "        for group in urls_found[:3]:\n",
    "            logger.info(f\"- {group['name']}: {group['group_url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:38:23,431 - __main__ - INFO - Page loaded, waiting for group elements...\n",
      "2025-05-19 22:38:24,586 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-05-19 22:38:26,706 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-05-19 22:38:28,825 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-05-19 22:38:30,942 - __main__ - INFO - Found 103 group elements\n",
      "2025-05-19 22:38:33,173 - __main__ - INFO - Saved 102 groups to herkey_groups.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended groups for Jane Doe:\n",
      "1. Women Engineers\n",
      "   Type: public | Members: 485\n",
      "   Category: development | Level: all\n",
      "   Featured: No\n",
      "\n",
      "2. Full Stack (MERN) Developer Program\n",
      "   Type: private | Members: N/A\n",
      "   Category: development | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "3. Ambassadors Club\n",
      "   Type: private | Members: 4129\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "4. HerRising\n",
      "   Type: public | Members: 39405\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n",
      "5. Open Ceiling Club\n",
      "   Type: private | Members: 134\n",
      "   Category: other | Level: all\n",
      "   Featured: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyGroupScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the group scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_groups(self, url: str = \"https://www.herkey.com/groups\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape group listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more groups)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of group dictionaries containing details of each group\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for group elements...\")\n",
    "            \n",
    "            # Wait for group elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='featured-group']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more groups\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get page source after all content is loaded\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            \n",
    "            # Find all group container elements\n",
    "            group_elements = soup.select(\"div.MuiGrid-container.css-1d3bbye\")\n",
    "            logger.info(f\"Found {len(group_elements)} group elements\")\n",
    "            \n",
    "            groups = []\n",
    "            for group_element in group_elements:\n",
    "                try:\n",
    "                    group = {}\n",
    "                    \n",
    "                    # Extract group name\n",
    "                    name_elem = group_element.select_one(\"[data-test-id='group-name']\")\n",
    "                    if name_elem:\n",
    "                        group[\"name\"] = name_elem.text.strip()\n",
    "                    \n",
    "                    # Extract group type (private/public)\n",
    "                    type_elem = group_element.select_one(\"[data-test-id='group-type']\")\n",
    "                    if type_elem:\n",
    "                        group[\"type\"] = type_elem.text.strip().lower()\n",
    "                    \n",
    "                    # Extract member count\n",
    "                    members_elem = group_element.select_one(\"[data-test-id='group-members-count']\")\n",
    "                    if members_elem:\n",
    "                        members_text = members_elem.text.strip()\n",
    "                        # Extract number from text like \"43 Members\"\n",
    "                        members_match = re.search(r'(\\d+)\\s+Members?', members_text)\n",
    "                        if members_match:\n",
    "                            group[\"member_count\"] = int(members_match.group(1))\n",
    "                    \n",
    "                    # Check if it's a featured group\n",
    "                    featured_elem = group_element.select_one(\"[data-test-id='featured-icon']\")\n",
    "                    group[\"featured\"] = featured_elem is not None\n",
    "                    \n",
    "                    # Extract group icon URL\n",
    "                    icon_elem = group_element.select_one(\"[data-test-id='group-icon']\")\n",
    "                    if icon_elem and icon_elem.has_attr('src'):\n",
    "                        group[\"icon_url\"] = icon_elem['src']\n",
    "                    \n",
    "                    # Extract banner image if available\n",
    "                    banner_elem = group_element.select_one(\".css-12c20jy img\")\n",
    "                    if banner_elem and banner_elem.has_attr('src'):\n",
    "                        group[\"banner_url\"] = banner_elem['src']\n",
    "                    \n",
    "                    # Extract group category/topic if available\n",
    "                    # Note: This might not be directly visible in the HTML snippet provided\n",
    "                    # We'll use a more generic approach to try and find it\n",
    "                    category_elem = group_element.select_one(\".MuiTypography-root.capitalize\")\n",
    "                    if category_elem:\n",
    "                        group[\"category\"] = category_elem.text.strip()\n",
    "                    \n",
    "                    # Extract join button status\n",
    "                    join_btn = group_element.select_one(\"[data-test-id='join-btn'] button\")\n",
    "                    if join_btn:\n",
    "                        group[\"joinable\"] = True\n",
    "                        group[\"join_button_text\"] = join_btn.text.strip()\n",
    "                    else:\n",
    "                        group[\"joinable\"] = False\n",
    "                    \n",
    "                    if \"name\" in group:  # Only add if we have at least a name\n",
    "                        groups.append(group)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting group data: {e}\")\n",
    "            \n",
    "            return groups\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, groups: List[Dict], filename: str = \"herkey_groups.json\"):\n",
    "        \"\"\"Save scraped groups to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(groups, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(groups)} groups to {filename}\")\n",
    "    \n",
    "    def process_groups_for_recommendation(self, groups: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process group data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes relevant fields\n",
    "        \"\"\"\n",
    "        processed_groups = []\n",
    "        \n",
    "        for group in groups:\n",
    "            processed_group = group.copy()\n",
    "            \n",
    "            # Extract topics/skills from group name where possible\n",
    "            name = group.get(\"name\", \"\").lower()\n",
    "            \n",
    "            # Extract potential skills/technologies\n",
    "            tech_keywords = [\n",
    "                \"javascript\", \"python\", \"java\", \"c++\", \"ruby\", \"php\", \"golang\", \"react\", \n",
    "                \"angular\", \"vue\", \"node\", \"express\", \"django\", \"flask\", \"spring\", \"bootstrap\",\n",
    "                \"html\", \"css\", \"sql\", \"nosql\", \"mongodb\", \"mysql\", \"postgresql\", \"oracle\",\n",
    "                \"aws\", \"azure\", \"gcp\", \"cloud\", \"devops\", \"data science\", \"machine learning\",\n",
    "                \"ai\", \"artificial intelligence\", \"blockchain\", \"iot\", \"mobile\", \"android\",\n",
    "                \"ios\", \"swift\", \"kotlin\", \"flutter\", \"react native\", \"full stack\", \"frontend\",\n",
    "                \"backend\", \"ui\", \"ux\", \"design\", \"product\", \"agile\", \"scrum\", \"kanban\",\n",
    "                \"mern\", \"mean\", \"lamp\", \"microservices\", \"docker\", \"kubernetes\", \"jenkins\",\n",
    "                \"ci/cd\", \"testing\", \"qa\", \"security\", \"cyber security\", \"data engineering\",\n",
    "                \"big data\", \"hadoop\", \"spark\", \"tableau\", \"power bi\", \"data visualization\"\n",
    "            ]\n",
    "            \n",
    "            # Look for tech keywords in the group name\n",
    "            found_keywords = [keyword for keyword in tech_keywords if keyword in name]\n",
    "            processed_group[\"tech_keywords\"] = found_keywords\n",
    "            \n",
    "            # Determine if group is for beginners, intermediate, or advanced\n",
    "            level_indicators = {\n",
    "                \"beginner\": [\"beginner\", \"basic\", \"fundamental\", \"101\", \"intro\", \"start\"],\n",
    "                \"intermediate\": [\"intermediate\", \"mid-level\"],\n",
    "                \"advanced\": [\"advanced\", \"expert\", \"professional\", \"master\", \"senior\"]\n",
    "            }\n",
    "            \n",
    "            for level, indicators in level_indicators.items():\n",
    "                if any(indicator in name for indicator in indicators):\n",
    "                    processed_group[\"level\"] = level\n",
    "                    break\n",
    "            else:\n",
    "                processed_group[\"level\"] = \"all\"  # Default if no level is detected\n",
    "            \n",
    "            # Add group category based on name (simplified)\n",
    "            if any(term in name for term in [\"developer\", \"coding\", \"programming\", \"engineer\", \"mern\", \"stack\"]):\n",
    "                processed_group[\"category\"] = \"development\"\n",
    "            elif any(term in name for term in [\"design\", \"ui\", \"ux\", \"user experience\"]):\n",
    "                processed_group[\"category\"] = \"design\"\n",
    "            elif any(term in name for term in [\"data\", \"analytics\", \"science\", \"machine learning\", \"ai\"]):\n",
    "                processed_group[\"category\"] = \"data_science\"\n",
    "            elif any(term in name for term in [\"management\", \"leader\", \"agile\", \"scrum\", \"product\"]):\n",
    "                processed_group[\"category\"] = \"management\"\n",
    "            elif any(term in name for term in [\"testing\", \"qa\", \"quality\"]):\n",
    "                processed_group[\"category\"] = \"testing\"\n",
    "            elif any(term in name for term in [\"devops\", \"cloud\", \"aws\", \"azure\", \"gcp\"]):\n",
    "                processed_group[\"category\"] = \"devops\"\n",
    "            else:\n",
    "                processed_group[\"category\"] = \"other\"\n",
    "            \n",
    "            processed_groups.append(processed_group)\n",
    "        \n",
    "        return processed_groups\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyGroupScraper(headless=False)  # Set to True for headless mode\n",
    "    groups = scraper.scrape_groups(max_scroll=3)\n",
    "    \n",
    "    # Save raw group data\n",
    "    scraper.save_to_json(groups)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 11:52:52,320 - __main__ - INFO - Page loaded, waiting for session elements...\n",
      "2025-05-26 11:52:52,342 - __main__ - INFO - Scrolling iteration 1/3...\n",
      "2025-05-26 11:52:54,357 - __main__ - INFO - Scrolling iteration 2/3...\n",
      "2025-05-26 11:52:56,368 - __main__ - INFO - Scrolling iteration 3/3...\n",
      "2025-05-26 11:52:58,407 - __main__ - INFO - Found 36 session listings\n",
      "2025-05-26 11:53:07,777 - __main__ - INFO - Saved 36 sessions to herkey_sessions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended sessions for John Doe:\n",
      "1. Lookups Unleashed: Your Ultimate Excel Guide\n",
      "   Date: 30 May 25 | Time: 2:00 PM\n",
      "   Host: Rajashree Das\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: ui, excel\n",
      "\n",
      "2. Power BI Master Class (Forage Project)\n",
      "   Date: 27 May 25 | Time: 10:30 AM\n",
      "   Host: Swati Agarwal\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: power bi\n",
      "\n",
      "3. Power BI master class (Forage project)\n",
      "   Date: Live | Time: N/A\n",
      "   Host: Swati Agarwal\n",
      "   Type: live | Category: general\n",
      "   Topics: power bi\n",
      "\n",
      "4. SQL Training for beginners by Intellipaat\n",
      "   Date: Today | Time: 1:00 PM\n",
      "   Host: Priya\n",
      "   Type: upcoming | Category: general\n",
      "   Topics: sql, ai\n",
      "\n",
      "5. SQL Training for beginners by Intellipaat\n",
      "   Date: 25 May 25 | Time: 1:00 PM\n",
      "   Host: Priya\n",
      "   Type: past | Category: general\n",
      "   Topics: sql, ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeySessionScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the session scraper with optional headless mode\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    def scrape_sessions(self, url: str = \"https://www.herkey.com/sessions\", max_scroll=5, scroll_pause=2) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape session listings from Herkey\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            max_scroll: Maximum number of scroll actions to perform (to load more sessions)\n",
    "            scroll_pause: Time to pause between scrolls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of session dictionaries containing details of each session\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            logger.info(\"Page loaded, waiting for session elements...\")\n",
    "            \n",
    "            # Wait for session elements to load\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"[datatestid='session-card']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll down to load more sessions\n",
    "            for i in range(max_scroll):\n",
    "                logger.info(f\"Scrolling iteration {i+1}/{max_scroll}...\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(scroll_pause)\n",
    "            \n",
    "            # Get all session cards\n",
    "            session_elements = driver.find_elements(By.CSS_SELECTOR, \"[datatestid='session-card'], [data-test-id='session-card']\")\n",
    "            logger.info(f\"Found {len(session_elements)} session listings\")\n",
    "            \n",
    "            sessions = []\n",
    "            for session_element in session_elements:\n",
    "                try:\n",
    "                    session = {}\n",
    "                    \n",
    "                    # Extract session time, date\n",
    "                    try:\n",
    "                        time_data = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='time-data']\").text\n",
    "                        if time_data:\n",
    "                            # Parse date and time (format: \"24 Apr 25 | 10:30 AM\")\n",
    "                            date_time_parts = time_data.split('|')\n",
    "                            if len(date_time_parts) > 0:\n",
    "                                session[\"date\"] = date_time_parts[0].strip()\n",
    "                            if len(date_time_parts) > 1:\n",
    "                                session[\"time\"] = date_time_parts[1].strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract time data\")\n",
    "                    \n",
    "                    # Extract session title\n",
    "                    try:\n",
    "                        discussion_subject = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-subject'] p\").text\n",
    "                        if discussion_subject:\n",
    "                            session[\"title\"] = discussion_subject\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session title\")\n",
    "                    \n",
    "                    # Extract host info\n",
    "                    try:\n",
    "                        host_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='nav-to-user-profile'] h6\")\n",
    "                        if host_element:\n",
    "                            session[\"host\"] = host_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host name\")\n",
    "                    \n",
    "                    # Extract host headline/role if available\n",
    "                    try:\n",
    "                        headline_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='headline']\")\n",
    "                        if headline_element and headline_element.text.strip():\n",
    "                            session[\"host_headline\"] = headline_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host headline\")\n",
    "                    \n",
    "                    # Get host stage/level\n",
    "                    try:\n",
    "                        stage_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-stage'] span\")\n",
    "                        if stage_element:\n",
    "                            session[\"host_stage\"] = stage_element.text\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract host stage\")\n",
    "                    \n",
    "                    # Get participant count\n",
    "                    try:\n",
    "                        participant_element = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='profile-pic']\")\n",
    "                        participant_text = participant_element.text\n",
    "                        if participant_text and participant_text.startswith(\"+\"):\n",
    "                            session[\"participant_count\"] = int(participant_text.replace(\"+\", \"\").strip())\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract participant count\")\n",
    "                    \n",
    "                    # Get session type (past/upcoming)\n",
    "                    try:\n",
    "                        session_type = session_element.get_attribute(\"data-sessiontype\")\n",
    "                        if session_type:\n",
    "                            session[\"session_type\"] = session_type\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session type\")\n",
    "                    \n",
    "                    # Get session ID\n",
    "                    try:\n",
    "                        session_id = session_element.get_attribute(\"data-id\")\n",
    "                        if session_id:\n",
    "                            session[\"id\"] = session_id\n",
    "                            # Construct URL\n",
    "                            if \"title\" in session:\n",
    "                                slug = session[\"title\"].lower().replace(\" \", \"-\")\n",
    "                                session[\"url\"] = f\"{url}/{slug}/{session_id}\"\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract session ID\")\n",
    "                    \n",
    "                    # Check if it's a video session\n",
    "                    try:\n",
    "                        video_element = session_element.find_element(By.CSS_SELECTOR, \"div[style*='youtube.com']\")\n",
    "                        if video_element:\n",
    "                            session[\"is_video\"] = True\n",
    "                            video_url = video_element.get_attribute(\"style\")\n",
    "                            # Extract YouTube video ID\n",
    "                            youtube_match = re.search(r'youtube.com/vi/([^/]+)/', video_url)\n",
    "                            if youtube_match:\n",
    "                                session[\"youtube_id\"] = youtube_match.group(1)\n",
    "                    except:\n",
    "                        session[\"is_video\"] = False\n",
    "                    \n",
    "                    # Get session status/action button text\n",
    "                    try:\n",
    "                        status_button = session_element.find_element(By.CSS_SELECTOR, \"[data-test-id='discussion-status-button'] button p\")\n",
    "                        if status_button:\n",
    "                            session[\"action\"] = status_button.text.strip()\n",
    "                    except:\n",
    "                        logger.debug(\"Could not extract action button text\")\n",
    "                    \n",
    "                    if session:  # Only add if we found some data\n",
    "                        sessions.append(session)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error extracting session data: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return sessions\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.error(\"Timeout waiting for page to load\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def save_to_json(self, sessions: List[Dict], filename: str = \"herkey_sessions.json\"):\n",
    "        \"\"\"Save scraped sessions to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sessions, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(sessions)} sessions to {filename}\")\n",
    "    \n",
    "    def process_sessions_for_recommendation(self, sessions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process session data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes dates, topics and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_sessions = []\n",
    "        \n",
    "        for session in sessions:\n",
    "            processed_session = session.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = session.get(\"date\", \"\")\n",
    "            time_str = session.get(\"time\", \"\")\n",
    "            datetime_str = f\"{date_str} {time_str}\".strip()\n",
    "\n",
    "            # In the process_sessions_for_recommendation method\n",
    "            try:\n",
    "                # Try various date formats\n",
    "                date_obj = None  # Initialize date_obj to avoid the UnboundLocalError\n",
    "                \n",
    "                if len(date_str.split()) == 3:  # Format: \"24 Apr 25\"\n",
    "                    date_obj = datetime.strptime(datetime_str, \"%d %b %y %I:%M %p\")\n",
    "                else:\n",
    "                    # Try other formats if the first one fails\n",
    "                    date_formats = [\n",
    "                        \"%d %b %Y %I:%M %p\",  # 24 Apr 2025 10:30 AM\n",
    "                        \"%d %B %Y %I:%M %p\",  # 24 April 2025 10:30 AM\n",
    "                        \"%b %d, %Y %I:%M %p\"  # Apr 24, 2025 10:30 AM\n",
    "                    ]\n",
    "                    \n",
    "                    for date_format in date_formats:\n",
    "                        try:\n",
    "                            date_obj = datetime.strptime(datetime_str, date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                # Check if date_obj was successfully set\n",
    "                if date_obj:\n",
    "                    processed_session[\"datetime_obj\"] = date_obj\n",
    "                    processed_session[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "                else:\n",
    "                    # Handle case where no format worked\n",
    "                    raise ValueError(\"Could not parse date string\")\n",
    "                    \n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, determine upcoming from session_type\n",
    "                if session.get(\"session_type\") == \"upcoming\":\n",
    "                    processed_session[\"is_upcoming\"] = True\n",
    "                else:\n",
    "                    processed_session[\"is_upcoming\"] = False\n",
    "            \n",
    "            # Extract topics from title\n",
    "            title = session.get(\"title\", \"\").lower()\n",
    "            topics = []\n",
    "            \n",
    "            # Common tech topics to check in the title\n",
    "            tech_topics = [\n",
    "                \"python\", \"javascript\", \"react\", \"angular\", \"vue\", \"node\", \"java\", \"c++\", \"c#\", \n",
    "                \"php\", \"ruby\", \"golang\", \"rust\", \"swift\", \"kotlin\", \"sql\", \"nosql\", \"mongodb\",\n",
    "                \"database\", \"cloud\", \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"devops\",\n",
    "                \"ai\", \"machine learning\", \"data science\", \"deep learning\", \"nlp\", \"computer vision\",\n",
    "                \"blockchain\", \"iot\", \"mobile\", \"web\", \"frontend\", \"backend\", \"fullstack\", \"ui\", \"ux\",\n",
    "                \"testing\", \"qa\", \"security\", \"agile\", \"scrum\", \"kanban\", \"product management\",\n",
    "                \"power bi\", \"tableau\", \"excel\", \"analytics\", \"big data\", \"hadoop\", \"spark\", \n",
    "                \"cybersecurity\", \"networking\", \"seo\", \"digital marketing\", \"dax\"\n",
    "            ]\n",
    "            \n",
    "            for topic in tech_topics:\n",
    "                if topic in title:\n",
    "                    topics.append(topic)\n",
    "            \n",
    "            processed_session[\"extracted_topics\"] = topics\n",
    "            \n",
    "            # Determine if session is technical or soft-skills\n",
    "            tech_indicators = [\"programming\", \"code\", \"developer\", \"software\", \"tech\", \"data\", \n",
    "                               \"engineering\", \"algorithm\", \"system\", \"database\", \"cloud\", \"devops\"]\n",
    "            \n",
    "            softskill_indicators = [\"career\", \"leadership\", \"management\", \"communication\",\n",
    "                                   \"soft skill\", \"interview\", \"resume\", \"cv\", \"personal\",\n",
    "                                   \"growth\", \"mindset\", \"wellbeing\", \"mental health\"]\n",
    "            \n",
    "            tech_score = sum(1 for indicator in tech_indicators if indicator in title)\n",
    "            softskill_score = sum(1 for indicator in softskill_indicators if indicator in title)\n",
    "            \n",
    "            if tech_score > softskill_score:\n",
    "                processed_session[\"session_category\"] = \"technical\"\n",
    "            elif softskill_score > tech_score:\n",
    "                processed_session[\"session_category\"] = \"soft skills\"\n",
    "            else:\n",
    "                processed_session[\"session_category\"] = \"general\"\n",
    "            \n",
    "            # Determine experience level based on title\n",
    "            if any(x in title for x in [\"beginner\", \"basic\", \"introduction\", \"101\", \"fundamentals\"]):\n",
    "                processed_session[\"experience_level\"] = \"beginner\"\n",
    "            elif any(x in title for x in [\"advanced\", \"expert\", \"mastery\", \"professional\"]):\n",
    "                processed_session[\"experience_level\"] = \"advanced\"\n",
    "            elif any(x in title for x in [\"intermediate\", \"part 2\", \"level 2\"]):\n",
    "                processed_session[\"experience_level\"] = \"intermediate\"\n",
    "            else:\n",
    "                processed_session[\"experience_level\"] = \"all levels\"\n",
    "            \n",
    "            processed_sessions.append(processed_session)\n",
    "        \n",
    "        return processed_sessions\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeySessionScraper(headless=False)  # Set to True for headless mode\n",
    "    sessions = scraper.scrape_sessions(max_scroll=3)\n",
    "    \n",
    "    # Save raw session data\n",
    "    scraper.save_to_json(sessions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a06a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:01:27,758 - __main__ - INFO - Scraping events from https://events.herkey.com/events\n",
      "c:\\Users\\knand\\OneDrive\\Desktop\\AshaAIBot\\asha\\Lib\\site-packages\\soupsieve\\css_parser.py:876: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(  # noqa: B028\n",
      "2025-05-26 12:01:29,640 - __main__ - INFO - Scraped 7 events\n",
      "2025-05-26 12:01:29,643 - __main__ - INFO - Saved 7 events to herkey_events.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended events for Jane Doe:\n",
      "1. Advance Your Career with a Premier Executive MBA from Great Lakes\n",
      "   Date: 16th Apr, 2025 to 31st May, 2025 | Time: 10:00am to 6:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development, event\n",
      "\n",
      "2. #STEMTheBias Scholarship - Avail Scholarships up to Rs. 80,000\n",
      "   Date: 3rd Mar, 2025 to 25th Mar, 2025 | Time: 10:00am to 11:59pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: Career Development\n",
      "\n",
      "3. SkillReBoot program: Restart Your Career Journey\n",
      "   Date: 21st Apr, 2025 to 30th Jun, 2025 | Time: 6:00pm to 11:00pm\n",
      "   Type: Online | Price: ₹ 1\n",
      "₹ 30000\n",
      "   Categories: Career Development\n",
      "\n",
      "4. HerFreshStart: Scholarships of up to 75% for Mothers Returning to Work\n",
      "   Date: 8th May, 2025 to 25th May, 2025 | Time: 3:43pm to 11:59pm\n",
      "   Type: Offline | Price: Free\n",
      "   Categories: Career Development, Diversity Drive\n",
      "\n",
      "5. herShakti\n",
      "   Date: 8th Mar, 2025 to 10th Apr, 2025 | Time: 12:00am to 11:00pm\n",
      "   Type: Online | Price: Free\n",
      "   Categories: JobsForHer foundation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HerkeyEventScraper:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the event scraper\"\"\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def _get_page_content(self, url: str) -> str:\n",
    "        \"\"\"Get HTML content from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def scrape_events(self, url: str = \"https://events.herkey.com/events\") -> List[Dict]:\n",
    "        \"\"\"Scrape events from HerKey events page.\"\"\"\n",
    "        logger.info(f\"Scraping events from {url}\")\n",
    "        html_content = self._get_page_content(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        events = []\n",
    "        \n",
    "        # Extract event listings based on the provided HTML structure\n",
    "        event_cards = soup.select(\".event-details-card\")\n",
    "        \n",
    "        for card in event_cards:\n",
    "            try:\n",
    "                event = {}\n",
    "                \n",
    "                # Extract event details based on the actual HTML structure\n",
    "                title_elem = card.select_one(\".card-heading\")\n",
    "                date_elem = card.select_one(\".card-body-data:has(img[src*='calendar'])\")\n",
    "                time_elem = card.select_one(\".card-body-data:has(img[src*='clock'])\")\n",
    "                location_elem = card.select_one(\".card-body-data:has(img[src*='placeholder'])\")\n",
    "                event_type_elem = card.select_one(\"span.mr-1, .card-body-data:has(i.fa-bullseye)\")\n",
    "                category_elem = card.select_one(\".card-body-data:has(img[src*='tag'])\")\n",
    "                \n",
    "                if title_elem:\n",
    "                    title_text = title_elem.get_text(strip=True)\n",
    "                    # Remove any icon text from the title\n",
    "                    if title_text:\n",
    "                        event[\"title\"] = title_text.split(\"Featured\")[0].strip()\n",
    "                \n",
    "                if date_elem:\n",
    "                    date_text = date_elem.get_text(strip=True)\n",
    "                    if date_text:\n",
    "                        event[\"date\"] = date_text\n",
    "                \n",
    "                if time_elem:\n",
    "                    time_text = time_elem.get_text(strip=True)\n",
    "                    if time_text:\n",
    "                        event[\"time\"] = time_text\n",
    "                \n",
    "                if location_elem:\n",
    "                    location_text = location_elem.get_text(strip=True)\n",
    "                    if location_text:\n",
    "                        event[\"location\"] = location_text\n",
    "                \n",
    "                if event_type_elem:\n",
    "                    event_type = event_type_elem.get_text(strip=True)\n",
    "                    if event_type and \"Online\" in event_type or \"Offline\" in event_type:\n",
    "                        event[\"event_type\"] = event_type\n",
    "                \n",
    "                if category_elem:\n",
    "                    # Parse categories (they're in a special format)\n",
    "                    category_links = category_elem.select(\"a\")\n",
    "                    if category_links:\n",
    "                        event[\"categories\"] = [link.text.strip() for link in category_links]\n",
    "                \n",
    "                # Get the registration button type\n",
    "                register_btn = card.select_one(\".register\")\n",
    "                if register_btn:\n",
    "                    event[\"registration_status\"] = \"Open\"\n",
    "                    event[\"registration_text\"] = register_btn.text.strip()\n",
    "                \n",
    "                # Check if it's a paid event\n",
    "                price_elem = card.select_one(\".card-body-data:contains('₹')\")\n",
    "                if price_elem:\n",
    "                    event[\"price\"] = price_elem.text.strip()\n",
    "                else:\n",
    "                    event[\"price\"] = \"Free\"\n",
    "                \n",
    "                # Get event ID and URL\n",
    "                event_id_span = card.select_one(\"span[id]\")\n",
    "                if event_id_span and event_id_span.get('id'):\n",
    "                    event_id = event_id_span.get('id')\n",
    "                    if \"title\" in event:\n",
    "                        event[\"url\"] = f\"{url}/{event['title'].lower().replace(' ', '-')}/{event_id}\"\n",
    "                \n",
    "                # Check if it's a featured event\n",
    "                featured_elem = card.select_one(\"img[src*='Featured'], img[width='22'][height='22']\")\n",
    "                if featured_elem:\n",
    "                    event[\"featured\"] = True\n",
    "                \n",
    "                if event:  # Only add if we found some data\n",
    "                    events.append(event)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting event data: {e}\")\n",
    "                \n",
    "        logger.info(f\"Scraped {len(events)} events\")\n",
    "        return events\n",
    "    \n",
    "    def save_to_json(self, events: List[Dict], filename: str = \"herkey_events.json\"):\n",
    "        \"\"\"Save scraped events to a JSON file\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(events, f, ensure_ascii=False, indent=4)\n",
    "        logger.info(f\"Saved {len(events)} events to {filename}\")\n",
    "    \n",
    "    def process_events_for_recommendation(self, events: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process event data to make it suitable for AI recommendation engine\n",
    "        Extracts and standardizes categories, dates and other relevant fields\n",
    "        \"\"\"\n",
    "        processed_events = []\n",
    "        \n",
    "        for event in events:\n",
    "            processed_event = event.copy()\n",
    "            \n",
    "            # Process date to datetime object if possible\n",
    "            date_str = event.get(\"date\", \"\")\n",
    "            try:\n",
    "                # Example format: \"25 Apr 2023\" - adjust pattern as needed\n",
    "                date_obj = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "                processed_event[\"datetime_obj\"] = date_obj\n",
    "                processed_event[\"is_upcoming\"] = date_obj > datetime.now()\n",
    "            except (ValueError, TypeError):\n",
    "                # If we can't parse the date, keep it as is\n",
    "                processed_event[\"is_upcoming\"] = True  # Assume upcoming by default\n",
    "            \n",
    "            # Standardize event type\n",
    "            event_type = event.get(\"event_type\", \"\").lower()\n",
    "            if \"online\" in event_type:\n",
    "                processed_event[\"mode\"] = \"online\"\n",
    "            elif \"offline\" in event_type or \"in-person\" in event_type:\n",
    "                processed_event[\"mode\"] = \"offline\"\n",
    "            else:\n",
    "                processed_event[\"mode\"] = \"unknown\"\n",
    "            \n",
    "            # Standardize categories\n",
    "            categories = event.get(\"categories\", [])\n",
    "            processed_event[\"categories_lower\"] = [cat.lower() for cat in categories]\n",
    "            \n",
    "            # Determine if event is free\n",
    "            price = event.get(\"price\", \"\")\n",
    "            processed_event[\"is_free\"] = price == \"Free\" or \"free\" in price.lower()\n",
    "            \n",
    "            processed_events.append(processed_event)\n",
    "        \n",
    "        return processed_events\n",
    "\n",
    "\n",
    "def recommend_events(candidate_profile: Dict, events: List[Dict], num_recommendations: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recommend events based on candidate profile\n",
    "    \n",
    "    Args:\n",
    "        candidate_profile: Dictionary containing candidate interests, career stage, etc.\n",
    "        events: List of event dictionaries\n",
    "        num_recommendations: Number of events to recommend\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended event dictionaries\n",
    "    \"\"\"\n",
    "    # Process events for recommendation if they haven't been processed\n",
    "    if events and \"categories_lower\" not in events[0]:\n",
    "        scraper = HerkeyEventScraper()\n",
    "        events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    scored_events = []\n",
    "    \n",
    "    # Get candidate preferences\n",
    "    interests = [interest.lower() for interest in candidate_profile.get(\"interests\", [])]\n",
    "    preferred_mode = candidate_profile.get(\"preferred_event_mode\", \"\").lower()\n",
    "    preferred_locations = [loc.lower() for loc in candidate_profile.get(\"preferred_locations\", [])]\n",
    "    career_stage = candidate_profile.get(\"career_stage\", \"\").lower()\n",
    "    \n",
    "    # Only consider upcoming events\n",
    "    upcoming_events = [event for event in events if event.get(\"is_upcoming\", True)]\n",
    "    \n",
    "    for event in upcoming_events:\n",
    "        score = 0\n",
    "        \n",
    "        # Match interests with event categories\n",
    "        event_categories = event.get(\"categories_lower\", [])\n",
    "        for interest in interests:\n",
    "            if any(interest in category for category in event_categories):\n",
    "                score += 10\n",
    "        \n",
    "        # Match event mode preference (online/offline)\n",
    "        event_mode = event.get(\"mode\", \"unknown\")\n",
    "        if preferred_mode and event_mode == preferred_mode:\n",
    "            score += 5\n",
    "        \n",
    "        # Match location preference if it's an offline event\n",
    "        if event_mode == \"offline\":\n",
    "            location = event.get(\"location\", \"\").lower()\n",
    "            if any(loc in location for loc in preferred_locations):\n",
    "                score += 5\n",
    "        \n",
    "        # Free events might be more appealing\n",
    "        if event.get(\"is_free\", False):\n",
    "            score += 2\n",
    "        \n",
    "        # Featured events might be more relevant/important\n",
    "        if event.get(\"featured\", False):\n",
    "            score += 3\n",
    "        \n",
    "        # Career stage specific events\n",
    "        event_title = event.get(\"title\", \"\").lower()\n",
    "        if career_stage in event_title or any(career_stage in cat for cat in event_categories):\n",
    "            score += 4\n",
    "        \n",
    "        scored_events.append((score, event))\n",
    "    \n",
    "    # Sort by score in descending order and take top recommendations\n",
    "    scored_events.sort(reverse=True, key=lambda x: x[0])\n",
    "    recommended_events = [event for score, event in scored_events[:num_recommendations]]\n",
    "    \n",
    "    return recommended_events\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    scraper = HerkeyEventScraper()\n",
    "    events = scraper.scrape_events()\n",
    "    \n",
    "    # Save raw event data\n",
    "    scraper.save_to_json(events)\n",
    "    \n",
    "    # Process data for recommendation engine\n",
    "    processed_events = scraper.process_events_for_recommendation(events)\n",
    "    \n",
    "    # Example candidate profile\n",
    "    example_candidate = {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"interests\": [\"Technology\", \"Career Development\", \"Leadership\", \"AI\"],\n",
    "        \"preferred_event_mode\": \"online\",\n",
    "        \"preferred_locations\": [\"Bangalore\", \"Mumbai\", \"Delhi\"],\n",
    "        \"career_stage\": \"mid-level\"\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommend_events(example_candidate, processed_events)\n",
    "    \n",
    "    print(f\"\\nRecommended events for {example_candidate['name']}:\")\n",
    "    for i, event in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {event['title']}\")\n",
    "        print(f\"   Date: {event['date']} | Time: {event.get('time', 'N/A')}\")\n",
    "        print(f\"   Type: {event.get('event_type', 'N/A')} | Price: {event.get('price', 'N/A')}\")\n",
    "        print(f\"   Categories: {', '.join(event.get('categories', []))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc96cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
